{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-29T18:13:38.552647Z",
     "start_time": "2025-01-29T18:13:38.548588Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import stats"
   ],
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T16:42:47.206320Z",
     "start_time": "2025-01-29T16:42:47.199629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_git_root():\n",
    "    try:\n",
    "        git_root = subprocess.check_output(['git', 'rev-parse', '--show-toplevel'],\n",
    "                                           stderr=subprocess.STDOUT).decode().strip()\n",
    "        return Path(git_root)\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"Warning: Not in a git repository. Using current working directory.\")\n",
    "        return Path.cwd()"
   ],
   "id": "c57f3e35772a182b",
   "outputs": [],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T16:42:47.251434Z",
     "start_time": "2025-01-29T16:42:47.224129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filename = 'Evaluating a Fact-Checking Process For Journalism.csv'\n",
    "output_dir = get_git_root() / 'user_study_effort'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created directory: {output_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {output_dir}\")\n",
    "df = pd.read_csv(filename)\n",
    "print(f'Number of answers pre filtering: {len(df)}')"
   ],
   "id": "f822246d46e848e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: /Users/sergiopinto/Desktop/MemeFact/user_study_effort\n",
      "Number of answers pre filtering: 100\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T16:42:47.268276Z",
     "start_time": "2025-01-29T16:42:47.264743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# filter invalid submissions on prolific (TIME TAKEN <= 10 minutes)\n",
    "\n",
    "rejected_prolific_ids = ['676e1a07c8eaac68bfb02b3d','6701920d56778252df2b1a49','6735e4faf027b9361e838666', '5c8d01cd2edaac00169007e6', '61092e5621c9bede90eb43b1', '67701371beadf0bff4672b32', '6786b7d4ca3e75e900edb6a5', '60690d928069052871e34f25', '667583f2ad1a0accaa279c25', '66aa9bfa8baa2b88248c0ed2', '673f8b0d3dd0d3cc8ca0fe32'\n",
    "]\n",
    "df = df[~df['Please enter your prolific ID.'].isin(rejected_prolific_ids)] \n",
    "print(f'Number of answers post rejection filtering: {len(df)}')"
   ],
   "id": "679e22839bbbd4c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of answers post rejection filtering: 90\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T16:42:47.283270Z",
     "start_time": "2025-01-29T16:42:47.280458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# filter duplicate answers\n",
    "\n",
    "df = df.drop_duplicates(subset=['Please enter your prolific ID.'])\n",
    "print(f'Number of answers post duplicate filtering: {len(df)}')"
   ],
   "id": "2ea74dab1d54ce5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of answers post duplicate filtering: 90\n"
     ]
    }
   ],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T16:42:47.306793Z",
     "start_time": "2025-01-29T16:42:47.303965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'Number of columns: {len(df)}')\n",
    "new_col_names = {\n",
    "    'Timestamp': 'timestamp',\n",
    "    'I hereby confirm that I have read the Data Collection & Privacy Information and consent to take part in this study by selecting the \\'I agree\\' option below:': 'auth',\n",
    "    'Please enter your prolific ID.': 'prolific_id'\n",
    "}\n",
    "df = df.rename(columns=new_col_names)"
   ],
   "id": "3afb157fe62f76c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 90\n"
     ]
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T16:42:47.327831Z",
     "start_time": "2025-01-29T16:42:47.323244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# participant private data\n",
    "\n",
    "new_col_names = {\n",
    "    'Please indicate your age group.': 'age_group',\n",
    "    'Please indicate your proficiency in English language comprehension.': 'english_level',\n",
    "    'Please indicate your highest completed level of education.': 'education_level',\n",
    "    'Please indicate your political orientation.': 'political_orientation',\n",
    "    'Please indicate your years of experience in journalistic fact checking and verification work.': 'fc_years_of_experience',\n",
    "    'Please provide your email address if you\\'d like to be contacted for future studies.': 'email'\n",
    "}\n",
    "columns_to_select = ['prolific_id'] + list(new_col_names.keys())\n",
    "participants_data = df[columns_to_select].rename(columns=new_col_names)\n",
    "output_path = f\"{output_dir}/participants_data.csv\"\n",
    "participants_data.to_csv(output_path, index=False)"
   ],
   "id": "6c5fb7b334e39e99",
   "outputs": [],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T16:42:47.356239Z",
     "start_time": "2025-01-29T16:42:47.352441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# participants feedback\n",
    "\n",
    "new_col_names = {\n",
    "    'Optional: Please share any additional thoughts or suggestions about the fact-checking process presented or any other feedback you may want to disclose.': 'process_feedback',\n",
    "    'Optional: Please share any additional observations, suggestions, or concerns about the questions and their justifications. Your feedback will help improve the fact-checking process.': 'claim_decomposition_feedback',\n",
    "    'Optional: Please share any additional observations, suggestions, or concerns about the question explanations and verdicts. Your feedback will help improve the fact-checking process.': 'evidence_synthesis_feedback',\n",
    "    'Optional: Please share any additional observations, suggestions, or concerns about the summary explanation and claim\\'s verdict. Your feedback will help improve the fact-checking process.': 'final_conclusion_feedback',\n",
    "    'Do you have any comments or suggestions about this survey? Your feedback will help us improve future studies.': 'user_study_feedback'\n",
    "}\n",
    "columns_to_select = ['prolific_id'] + list(new_col_names.keys())\n",
    "participants_feedback = df[columns_to_select].rename(columns=new_col_names)\n",
    "output_path = f\"{output_dir}/participants_feedback.csv\"\n",
    "participants_feedback.to_csv(output_path, index=False)"
   ],
   "id": "86526b7b047d4851",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T16:42:47.363486Z",
     "start_time": "2025-01-29T16:42:47.360614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# process evaluation\n",
    "\n",
    "new_col_names = {\n",
    "    'How would you assess the explainability of the fact-checking process\\'s intermediate steps in terms of understanding how claims are broken down and analyzed to reach verdicts and generate explanations?': 'process_explainability',\n",
    "    'How would you assess the transparency of the process in demonstrating the progression from input claim to verdict and explanation summary?': 'process_transparency',\n",
    "    'How would you assess the transparency in how sources are selected and validated?': 'sources_transparency',\n",
    "    'How would you assess the level of trust that this fact-checking process inspires?': 'process_level_of_trust',\n",
    "    'How would you assess the credibility of this fact-checking process compared to existing fact-checking approaches you are familiar with? ': 'process_credibility'\n",
    "}\n",
    "columns_to_select = ['prolific_id'] + list(new_col_names.keys())\n",
    "process_evaluation = df[columns_to_select].rename(columns=new_col_names)\n",
    "output_path = f\"{output_dir}/process_evaluation.csv\"\n",
    "process_evaluation.to_csv(output_path, index=False)"
   ],
   "id": "4d02d2d8b72b8cc8",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T16:42:47.378318Z",
     "start_time": "2025-01-29T16:42:47.374099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cd artifacts evaluation\n",
    "\n",
    "new_col_names = {\n",
    "    'How thoroughly do the decomposing questions cover all aspects of the claim that need verification?': 'questions_coverage',\n",
    "    'How relevant are the additional aspects introduced by the decomposing questions for verifying the claim\\'s accuracy?': 'questions_relevance',\n",
    "    'How well are the questions formulated to allow for clear \"Yes\", \"No\", or \"Unverified\" verdicts based on available evidence?': 'questions_formulation',\n",
    "    'How well do the justifications explain the relevance of the questions for verifying the claim\\'s accuracy?': 'justifications_explainability'\n",
    "}\n",
    "\n",
    "rename_mapping = {}\n",
    "\n",
    "for old_col, new_base in new_col_names.items():\n",
    "    rename_mapping[old_col] = f\"{new_base}_claim1\"\n",
    "\n",
    "for i in range(1, 5):\n",
    "    for old_col, new_base in new_col_names.items():\n",
    "        old_col_with_suffix = f\"{old_col}.{i}\"\n",
    "        rename_mapping[old_col_with_suffix] = f\"{new_base}_claim{i+1}\"\n",
    "\n",
    "columns_to_select = ['prolific_id'] + list(rename_mapping.keys())\n",
    "cd_artifacts_evaluation = df[columns_to_select].rename(columns=rename_mapping)\n",
    "\n",
    "output_path = f\"{output_dir}/cd_artifacts_evaluation.csv\"\n",
    "cd_artifacts_evaluation.to_csv(output_path, index=False)"
   ],
   "id": "2e1b81827bcf87f9",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T16:42:47.392959Z",
     "start_time": "2025-01-29T16:42:47.388837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# es artifacts evaluation\n",
    "\n",
    "new_col_names = {\n",
    "   'How relevant are the explanations to answering their respective questions?': 'explanations_relevance',\n",
    "   'How effective are the explanations at reaching conclusions that respond to the questions?': 'explanations_effectiveness', \n",
    "   'How logically do the verdicts follow from the evidence in the explanations given the questions?': 'verdicts_logical_connection'\n",
    "}\n",
    "\n",
    "rename_mapping = {}\n",
    "\n",
    "for old_col, new_base in new_col_names.items():\n",
    "   rename_mapping[old_col] = f\"{new_base}_claim1\"\n",
    "\n",
    "for i in range(1, 5):\n",
    "   for old_col, new_base in new_col_names.items():\n",
    "       old_col_with_suffix = f\"{old_col}.{i}\"\n",
    "       rename_mapping[old_col_with_suffix] = f\"{new_base}_claim{i+1}\"\n",
    "\n",
    "columns_to_select = ['prolific_id'] + list(rename_mapping.keys())\n",
    "es_artifacts_evaluation = df[columns_to_select].rename(columns=rename_mapping)\n",
    "\n",
    "output_path = f\"{output_dir}/es_artifacts_evaluation.csv\"\n",
    "es_artifacts_evaluation.to_csv(output_path, index=False)"
   ],
   "id": "891265ac02131619",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T16:42:47.407555Z",
     "start_time": "2025-01-29T16:42:47.403313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# fc artifacts evaluation\n",
    "\n",
    "new_col_names = {\n",
    "   'How thoroughly does the summary explanation cover all aspects of the claim that need verification?': 'summary_coverage',\n",
    "   'How well does the summary explanation support the final verdict given to the claim?': 'summary_verdict_support',\n",
    "   'How factually aligned is Summary 1 with Summary 2?': 'summaries_allignment',\n",
    "   'As a journalist, which summary would you consider more credible and rigorous for fact-checking the claim?': 'summary_choice'\n",
    "}\n",
    "\n",
    "rename_mapping = {}\n",
    "\n",
    "for old_col, new_base in new_col_names.items():\n",
    "   rename_mapping[old_col] = f\"{new_base}_claim1\"\n",
    "\n",
    "for i in range(1, 5):\n",
    "   for old_col, new_base in new_col_names.items():\n",
    "       old_col_with_suffix = f\"{old_col}.{i}\"\n",
    "       rename_mapping[old_col_with_suffix] = f\"{new_base}_claim{i+1}\"\n",
    "\n",
    "columns_to_select = ['prolific_id'] + list(rename_mapping.keys())\n",
    "fc_artifacts_evaluation = df[columns_to_select].rename(columns=rename_mapping)\n",
    "\n",
    "output_path = f\"{output_dir}/fc_artifacts_evaluation.csv\"\n",
    "fc_artifacts_evaluation.to_csv(output_path, index=False)"
   ],
   "id": "d8e2e4c7b7e0afd3",
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T17:00:13.575802Z",
     "start_time": "2025-01-29T17:00:13.547604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "claims_data = [\n",
    "   {\n",
    "       'number': 1,\n",
    "       'text': 'Says Ford agreed to invest $900 million at an Ohio plant because Donald Trump lowered taxes and is now moving the project to Mexico because Joe Biden is increasing taxes.',\n",
    "       'political_stance': 'pro_republican',\n",
    "       'evidence_id': '-1365092868303902720'\n",
    "   },\n",
    "   {\n",
    "       'number': 2,\n",
    "       'text': 'The Biden administration inherited gains of 50,000 jobs a month. We\\'re now finally back to 500,000 jobs a month. We inherited a country where 4,000 people a day were dying from Covid. That\\'s now down 75%.',\n",
    "       'political_stance': 'pro_democrat', \n",
    "       'evidence_id': '5948245359632679936'\n",
    "   },\n",
    "   {\n",
    "       'number': 3,\n",
    "       'text': 'Officials recommend that women who get one of these (COVID-19) shots should absolutely not get pregnant for at least the first two months after they\\'ve been injected.',\n",
    "       'political_stance': 'neutral',\n",
    "       'evidence_id': '1015833665715298432'\n",
    "   },\n",
    "   {\n",
    "       'number': 4,\n",
    "       'text': 'Joe Biden and Kamala Harris government-run health care plan could lead to hospitals being closed, put Medicare coverage at risk, and give benefits to illegal immigrants.',\n",
    "       'political_stance': 'pro_republican',\n",
    "       'evidence_id': '7928069865906505728'\n",
    "   },\n",
    "   {\n",
    "       'number': 5,\n",
    "       'text': 'Wisconsin was the last state to start paying COVID-related federal unemployment benefits.',\n",
    "       'political_stance': 'neutral',\n",
    "       'evidence_id': '8719092942913588224'\n",
    "   }\n",
    "]\n",
    "\n",
    "claims = pd.DataFrame(claims_data)\n",
    "\n",
    "output_path = f\"{output_dir}/claims.csv\"\n",
    "claims.to_csv(output_path, index=False)"
   ],
   "id": "a187bd0221200f0b",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "processed_dir = os.path.join(output_dir, 'processed')\n",
    "if not os.path.exists(processed_dir):\n",
    "   os.makedirs(processed_dir)"
   ],
   "id": "b3e3ca81a457797c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T18:08:49.261151Z",
     "start_time": "2025-01-29T18:08:49.234085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# process evaluations processing\n",
    "\n",
    "explainability_map = {\n",
    "    'No clear explainability': 1,\n",
    "    'Limited explainability': 2,\n",
    "    'Adequate explainability': 3,\n",
    "    'High explainability': 4,\n",
    "    'Exceptional explainability': 5\n",
    "}\n",
    "\n",
    "process_transparency_map = {\n",
    "    'No clear transparency': 1,\n",
    "    'Limited transparency': 2,\n",
    "    'Adequate transparency': 3,\n",
    "    'High transparency': 4,\n",
    "    'Exceptional transparency': 5\n",
    "}\n",
    "\n",
    "sources_transparency_map = {\n",
    "    'No clear transparency': 1,\n",
    "    'Limited transparency': 2,\n",
    "    'Adequate transparency': 3,\n",
    "    'High transparency': 4,\n",
    "    'Exceptional transparency': 5\n",
    "}\n",
    "\n",
    "trust_map = {\n",
    "    'No clear trustworthiness': 1,\n",
    "    'Limited trustworthiness': 2,\n",
    "    'Adequate trustworthiness': 3,\n",
    "    'High trustworthiness': 4,\n",
    "    'Exceptional trustworthiness': 5\n",
    "}\n",
    "\n",
    "credibility_map = {\n",
    "    'No prior experience with fact-checking systems': 0,\n",
    "    'Lower credibility': 1,\n",
    "    'Similar credibility': 2,\n",
    "    'Superior credibility': 3\n",
    "}\n",
    "\n",
    "df = pd.read_csv(f\"{output_dir}/process_evaluation.csv\")\n",
    "\n",
    "process_evaluation = df.copy()\n",
    "\n",
    "process_evaluation['process_explainability'] = process_evaluation['process_explainability'].map(explainability_map)\n",
    "process_evaluation['process_transparency'] = process_evaluation['process_transparency'].map(process_transparency_map)\n",
    "process_evaluation['sources_transparency'] = process_evaluation['sources_transparency'].map(sources_transparency_map)\n",
    "process_evaluation['process_level_of_trust'] = process_evaluation['process_level_of_trust'].map(trust_map)\n",
    "process_evaluation['process_credibility'] = process_evaluation['process_credibility'].map(credibility_map)\n",
    "\n",
    "# Save to processed directory\n",
    "output_path = os.path.join(processed_dir, 'process_evaluation_phase_1.csv')\n",
    "process_evaluation.to_csv(output_path, index=False)"
   ],
   "id": "9400f59f725771d",
   "outputs": [],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T17:16:46.796914Z",
     "start_time": "2025-01-29T17:16:46.774846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cd artifacts processing\n",
    "\n",
    "coverage_map = {\n",
    "   'No coverage': 1,\n",
    "   'Limited coverage': 2,\n",
    "   'Adequate coverage': 3, \n",
    "   'High coverage': 4,\n",
    "   'Excellent coverage': 5\n",
    "}\n",
    "\n",
    "relevance_map = {\n",
    "   'No relevance': 1,\n",
    "   'Limited relevance': 2,\n",
    "   'Adequate relevance': 3,\n",
    "   'High relevance': 4, \n",
    "   'Excellent relevance': 5\n",
    "}\n",
    "\n",
    "formulation_map = {\n",
    "   'Poor formulation': 1,\n",
    "   'Limited formulation': 2,\n",
    "   'Adequate formulation': 3,\n",
    "   'Strong formulation': 4,\n",
    "   'Excellent formulation': 5\n",
    "}\n",
    "\n",
    "justification_map = {\n",
    "   'No relevance': 1,\n",
    "   'Limited relevance': 2,\n",
    "   'Adequate relevance': 3,\n",
    "   'High relevance': 4,\n",
    "   'Excellent relevance': 5\n",
    "}\n",
    "\n",
    "cd_df = pd.read_csv(f\"{output_dir}/cd_artifacts_evaluation.csv\")\n",
    "cd_processed = cd_df.copy()\n",
    "\n",
    "for col in cd_processed.columns:\n",
    "   if 'questions_coverage' in col:\n",
    "       cd_processed[col] = cd_processed[col].map(coverage_map)\n",
    "   elif 'questions_relevance' in col:\n",
    "       cd_processed[col] = cd_processed[col].map(relevance_map)\n",
    "   elif 'questions_formulation' in col:\n",
    "       cd_processed[col] = cd_processed[col].map(formulation_map)\n",
    "   elif 'justifications_explainability' in col:\n",
    "       cd_processed[col] = cd_processed[col].map(justification_map)\n",
    "   else:\n",
    "       print(f'Not processing column: {col}')\n",
    "\n",
    "output_path = os.path.join(processed_dir, 'cd_process_phase_1.csv')\n",
    "cd_processed.to_csv(output_path, index=False)"
   ],
   "id": "69d00dc83ce72632",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not processing column: prolific_id\n"
     ]
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T17:21:49.111182Z",
     "start_time": "2025-01-29T17:21:49.079057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# es artifacts processing\n",
    "\n",
    "relevance_map = {\n",
    "   'No relevance': 1,\n",
    "   'Limited relevance': 2,\n",
    "   'Adequate relevance': 3,\n",
    "   'High relevance': 4, \n",
    "   'Excellent relevance': 5\n",
    "}\n",
    "\n",
    "effectiveness_map = {\n",
    "   'No effectiveness': 1,\n",
    "   'Limited effectiveness': 2,\n",
    "   'Adequate effectiveness': 3,\n",
    "   'High effectiveness': 4,\n",
    "   'Excellent effectiveness': 5\n",
    "}\n",
    "\n",
    "logical_connection_map = {\n",
    "   'No logical connection': 1,\n",
    "   'Limited logical connection': 2,\n",
    "   'Adequate logical connection': 3,\n",
    "   'Strong logical connection': 4,\n",
    "   'Excellent logical connection': 5\n",
    "}\n",
    "\n",
    "es_df = pd.read_csv(f\"{output_dir}/es_artifacts_evaluation.csv\")\n",
    "es_processed = es_df.copy()\n",
    "\n",
    "for col in es_processed.columns:\n",
    "   if 'explanations_relevance' in col:\n",
    "       es_processed[col] = es_processed[col].map(relevance_map)\n",
    "   elif 'explanations_effectiveness' in col:\n",
    "       es_processed[col] = es_processed[col].map(effectiveness_map)\n",
    "   elif 'verdicts_logical_connection' in col:\n",
    "       es_processed[col] = es_processed[col].map(logical_connection_map)\n",
    "   else:\n",
    "       print(f'Not processing column: {col}')\n",
    "\n",
    "output_path = os.path.join(processed_dir, 'es_process_phase_1.csv')\n",
    "es_processed.to_csv(output_path, index=False)"
   ],
   "id": "b2452d9871394088",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not processing column: prolific_id\n"
     ]
    }
   ],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T17:29:56.354720Z",
     "start_time": "2025-01-29T17:29:56.320591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# fc artifacts processing\n",
    "\n",
    "coverage_map = {\n",
    "   'No coverage': 1,\n",
    "   'Limited coverage': 2,\n",
    "   'Adequate coverage': 3,\n",
    "   'High coverage': 4, \n",
    "   'Excellent coverage': 5\n",
    "}\n",
    "\n",
    "support_map = {\n",
    "   'No support': 1,\n",
    "   'Limited support': 2,\n",
    "   'Adequate support': 3,\n",
    "   'Strong support': 4,\n",
    "   'Excellent support': 5\n",
    "}\n",
    "\n",
    "alignment_map = {\n",
    "   'Completely misaligned (Contains fabricated content that completely alters the meaning)': 1,\n",
    "   'Major misalignment (Contains factual errors that significantly misrepresent the content)': 2,\n",
    "   'Minor misalignment (Some details differ but maintains the overall message)': 3,\n",
    "   'Completely aligned (Accurately represents the same meaning and details)': 4\n",
    "}\n",
    "\n",
    "choice_map = {\n",
    "   'Summary 1 is significantly more credible and rigorous': 5,\n",
    "   'Summary 1 is somewhat more credible and rigorous': 4,\n",
    "   'Both summaries are equally credible and rigorous': 3,\n",
    "   'Summary 2 is somewhat more credible and rigorous': 2,\n",
    "   'Summary 2 is significantly more credible and rigorous': 1\n",
    "}\n",
    "\n",
    "fc_df = pd.read_csv(f\"{output_dir}/fc_artifacts_evaluation.csv\")\n",
    "fc_processed = fc_df.copy()\n",
    "\n",
    "for col in fc_processed.columns:\n",
    "   if 'summary_coverage' in col:\n",
    "       fc_processed[col] = fc_processed[col].map(coverage_map)\n",
    "   elif 'summary_verdict_support' in col:\n",
    "       fc_processed[col] = fc_processed[col].map(support_map)\n",
    "   elif 'summaries_allignment' in col:\n",
    "       fc_processed[col] = fc_processed[col].map(alignment_map)\n",
    "   elif 'summary_choice' in col:\n",
    "       fc_processed[col] = fc_processed[col].map(choice_map)\n",
    "   else:\n",
    "       print(f'Not processing column: {col}')\n",
    "\n",
    "output_path = os.path.join(processed_dir, 'fc_process_phase_1.csv')\n",
    "fc_processed.to_csv(output_path, index=False)"
   ],
   "id": "daf38ba44a102d34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not processing column: prolific_id\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T20:35:21.181967Z",
     "start_time": "2025-01-29T20:35:21.155196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import scipy\n",
    "\n",
    "proc_df = pd.read_csv(f\"{output_dir}/processed/process_evaluation_phase_1.csv\")\n",
    "\n",
    "def get_comprehensive_stats(values, scale_max):\n",
    "   mean = np.mean(values)\n",
    "   std = np.std(values)\n",
    "   n = len(values)\n",
    "   se = scipy.stats.sem(values)\n",
    "   ci = scipy.stats.t.interval(confidence=0.95, df=n-1, loc=mean, scale=se)\n",
    "   \n",
    "   # Value distribution\n",
    "   value_counts = pd.Series(values).value_counts().sort_index()\n",
    "   percentages = (value_counts / len(values) * 100).round(2)\n",
    "   \n",
    "   return {\n",
    "       'basic_stats': {\n",
    "           'mean': mean,\n",
    "           'std': std,\n",
    "           'median': np.median(values),\n",
    "           'min': np.min(values),\n",
    "           'max': np.max(values),\n",
    "           'q1': np.percentile(values, 25),\n",
    "           'q3': np.percentile(values, 75),\n",
    "           'ci_lower': ci[0],\n",
    "           'ci_upper': ci[1],\n",
    "           'mean_percentage': (mean/scale_max)*100,\n",
    "           'n': n\n",
    "       },\n",
    "       'distribution': {\n",
    "           'values': value_counts.index.tolist(),\n",
    "           'counts': value_counts.tolist(),\n",
    "           'percentages': percentages.tolist()\n",
    "       },\n",
    "       'raw_values': values\n",
    "   }\n",
    "\n",
    "dimensions_5scale = {\n",
    "   'Process Explainability': {'col': 'process_explainability', 'stats': None},\n",
    "   'Process Transparency': {'col': 'process_transparency', 'stats': None},\n",
    "   'Sources Transparency': {'col': 'sources_transparency', 'stats': None},\n",
    "   'Process Level of Trust': {'col': 'process_level_of_trust', 'stats': None}\n",
    "}\n",
    "\n",
    "for dim_name, dim_data in dimensions_5scale.items():\n",
    "   dim_data['stats'] = get_comprehensive_stats(proc_df[dim_data['col']].values, 5)\n",
    "\n",
    "def print_basic_stats(stats, dimension, scale_max):\n",
    "   bs = stats['basic_stats']\n",
    "   print(f\"\\n{dimension} Statistics (Scale 1-{scale_max}):\")\n",
    "   print(f\"Mean: {bs['mean']:.2f} (95% CI: [{bs['ci_lower']:.2f}, {bs['ci_upper']:.2f}])\")\n",
    "   print(f\"Standard Deviation: {bs['std']:.2f}\")\n",
    "   print(f\"Median: {bs['median']:.2f}\")\n",
    "   print(f\"Min: {bs['min']:.2f}\")\n",
    "   print(f\"Max: {bs['max']:.2f}\")\n",
    "   print(f\"Q1: {bs['q1']:.2f}\")\n",
    "   print(f\"Q3: {bs['q3']:.2f}\")\n",
    "   print(f\"Mean as % of maximum: {bs['mean_percentage']:.1f}%\")\n",
    "\n",
    "def print_distribution(stats, dimension):\n",
    "   dist = stats['distribution']\n",
    "   print(f\"\\n{dimension} Value Distribution:\")\n",
    "   for val, count, pct in zip(dist['values'], dist['counts'], dist['percentages']):\n",
    "       print(f\"Value {val}: {count} occurrences ({pct:.1f}%)\")\n",
    "\n",
    "for dim_name, dim_data in dimensions_5scale.items():\n",
    "   print_basic_stats(dim_data['stats'], dim_name, 5)\n",
    "   print_distribution(dim_data['stats'], dim_name)\n",
    "\n",
    "credibility_data = proc_df['process_credibility']\n",
    "zero_count = (credibility_data == 0).sum()\n",
    "total_count = len(credibility_data)\n",
    "zero_percentage = (zero_count / total_count) * 100\n",
    "\n",
    "print(\"\\nProcess Credibility Analysis:\")\n",
    "print(f\"Participants with no prior experience: {zero_count} ({zero_percentage:.1f}%)\")\n",
    "\n",
    "non_zero_data = credibility_data[credibility_data != 0]\n",
    "if len(non_zero_data) > 0:\n",
    "   credibility_stats = get_comprehensive_stats(non_zero_data, 3)\n",
    "   bs = credibility_stats['basic_stats']\n",
    "   \n",
    "   print(\"\\nStatistics for participants with prior experience (Scale 1-3):\")\n",
    "   print(f\"N = {bs['n']} participants\")\n",
    "   print(f\"Mean: {bs['mean']:.2f} (95% CI: [{bs['ci_lower']:.2f}, {bs['ci_upper']:.2f}])\")\n",
    "   print(f\"Standard Deviation: {bs['std']:.2f}\")\n",
    "   print(f\"Median: {bs['median']:.2f}\")\n",
    "   print(f\"Min: {bs['min']:.2f}\")\n",
    "   print(f\"Max: {bs['max']:.2f}\")\n",
    "   print(f\"Q1: {bs['q1']:.2f}\")\n",
    "   print(f\"Q3: {bs['q3']:.2f}\")\n",
    "   print(f\"Mean as % of maximum: {bs['mean_percentage']:.1f}%\")\n",
    "   print_distribution(credibility_stats, \"Process Credibility (excluding no prior experience)\")"
   ],
   "id": "4df8bcd9071b48c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Process Explainability Statistics (Scale 1-5):\n",
      "Mean: 3.60 (95% CI: [3.44, 3.76])\n",
      "Standard Deviation: 0.77\n",
      "Median: 4.00\n",
      "Min: 1.00\n",
      "Max: 5.00\n",
      "Q1: 3.00\n",
      "Q3: 4.00\n",
      "Mean as % of maximum: 72.0%\n",
      "\n",
      "Process Explainability Value Distribution:\n",
      "Value 1: 1 occurrences (1.1%)\n",
      "Value 2: 6 occurrences (6.7%)\n",
      "Value 3: 28 occurrences (31.1%)\n",
      "Value 4: 48 occurrences (53.3%)\n",
      "Value 5: 7 occurrences (7.8%)\n",
      "\n",
      "Process Transparency Statistics (Scale 1-5):\n",
      "Mean: 3.54 (95% CI: [3.38, 3.71])\n",
      "Standard Deviation: 0.78\n",
      "Median: 4.00\n",
      "Min: 2.00\n",
      "Max: 5.00\n",
      "Q1: 3.00\n",
      "Q3: 4.00\n",
      "Mean as % of maximum: 70.9%\n",
      "\n",
      "Process Transparency Value Distribution:\n",
      "Value 2: 9 occurrences (10.0%)\n",
      "Value 3: 30 occurrences (33.3%)\n",
      "Value 4: 44 occurrences (48.9%)\n",
      "Value 5: 7 occurrences (7.8%)\n",
      "\n",
      "Sources Transparency Statistics (Scale 1-5):\n",
      "Mean: 3.24 (95% CI: [3.07, 3.42])\n",
      "Standard Deviation: 0.83\n",
      "Median: 3.00\n",
      "Min: 1.00\n",
      "Max: 5.00\n",
      "Q1: 3.00\n",
      "Q3: 4.00\n",
      "Mean as % of maximum: 64.9%\n",
      "\n",
      "Sources Transparency Value Distribution:\n",
      "Value 1: 2 occurrences (2.2%)\n",
      "Value 2: 12 occurrences (13.3%)\n",
      "Value 3: 43 occurrences (47.8%)\n",
      "Value 4: 28 occurrences (31.1%)\n",
      "Value 5: 5 occurrences (5.6%)\n",
      "\n",
      "Process Level of Trust Statistics (Scale 1-5):\n",
      "Mean: 3.63 (95% CI: [3.48, 3.79])\n",
      "Standard Deviation: 0.72\n",
      "Median: 4.00\n",
      "Min: 2.00\n",
      "Max: 5.00\n",
      "Q1: 3.00\n",
      "Q3: 4.00\n",
      "Mean as % of maximum: 72.7%\n",
      "\n",
      "Process Level of Trust Value Distribution:\n",
      "Value 2: 6 occurrences (6.7%)\n",
      "Value 3: 28 occurrences (31.1%)\n",
      "Value 4: 49 occurrences (54.4%)\n",
      "Value 5: 7 occurrences (7.8%)\n",
      "\n",
      "Process Credibility Analysis:\n",
      "Participants with no prior experience: 5 (5.6%)\n",
      "\n",
      "Statistics for participants with prior experience (Scale 1-3):\n",
      "N = 85 participants\n",
      "Mean: 2.20 (95% CI: [2.07, 2.33])\n",
      "Standard Deviation: 0.59\n",
      "Median: 2.00\n",
      "Min: 1.00\n",
      "Max: 3.00\n",
      "Q1: 2.00\n",
      "Q3: 3.00\n",
      "Mean as % of maximum: 73.3%\n",
      "\n",
      "Process Credibility (excluding no prior experience) Value Distribution:\n",
      "Value 1: 8 occurrences (9.4%)\n",
      "Value 2: 52 occurrences (61.2%)\n",
      "Value 3: 25 occurrences (29.4%)\n"
     ]
    }
   ],
   "execution_count": 165
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T18:50:17.438928Z",
     "start_time": "2025-01-29T18:50:17.401143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cd artifacts statistics\n",
    "cd_df = pd.read_csv(f\"{output_dir}/processed/cd_process_phase_1.csv\")\n",
    "\n",
    "coverage_cols = [col for col in cd_df.columns if 'questions_coverage' in col]\n",
    "relevance_cols = [col for col in cd_df.columns if 'questions_relevance' in col]\n",
    "formulation_cols = [col for col in cd_df.columns if 'questions_formulation' in col]\n",
    "explainability_cols = [col for col in cd_df.columns if 'justifications_explainability' in col]\n",
    "\n",
    "def get_comprehensive_stats(df, cols):\n",
    "    all_values = df[cols].values.ravel()\n",
    "    \n",
    "    mean = np.mean(all_values)\n",
    "    std = np.std(all_values)\n",
    "    n = len(all_values)\n",
    "    se = scipy.stats.sem(all_values)\n",
    "    ci = scipy.stats.t.interval(confidence=0.95, df=n-1, loc=mean, scale=se)\n",
    "    \n",
    "    value_counts = pd.Series(all_values).value_counts().sort_index()\n",
    "    percentages = (value_counts / len(all_values) * 100).round(2)\n",
    "    \n",
    "    return {\n",
    "        'basic_stats': {\n",
    "            'mean': mean,\n",
    "            'std': std,\n",
    "            'median': np.median(all_values),\n",
    "            'min': np.min(all_values),\n",
    "            'max': np.max(all_values),\n",
    "            'q1': np.percentile(all_values, 25),\n",
    "            'q3': np.percentile(all_values, 75),\n",
    "            'ci_lower': ci[0],\n",
    "            'ci_upper': ci[1]\n",
    "        },\n",
    "        'distribution': {\n",
    "            'values': value_counts.index.tolist(),\n",
    "            'counts': value_counts.tolist(),\n",
    "            'percentages': percentages.tolist()\n",
    "        },\n",
    "        'raw_values': all_values\n",
    "    }\n",
    "\n",
    "dimensions = {\n",
    "    'Question Coverage': {'cols': coverage_cols, 'stats': None},\n",
    "    'Question Relevance': {'cols': relevance_cols, 'stats': None},\n",
    "    'Question Formulation': {'cols': formulation_cols, 'stats': None},\n",
    "    'Justification Explainability': {'cols': explainability_cols, 'stats': None}\n",
    "}\n",
    "\n",
    "for dim_name, dim_data in dimensions.items():\n",
    "    dim_data['stats'] = get_comprehensive_stats(cd_df, dim_data['cols'])\n",
    "\n",
    "claim_stats = {}\n",
    "for i in range(1, 6):\n",
    "    claim_cols = [col for col in cd_df.columns if f'claim{i}' in col]\n",
    "    claim_stats[f'claim_{i}'] = get_comprehensive_stats(cd_df, claim_cols)\n",
    "\n",
    "def print_basic_stats(stats, dimension):\n",
    "    bs = stats['basic_stats']\n",
    "    print(f\"\\n{dimension} Statistics:\")\n",
    "    print(f\"Mean: {bs['mean']:.2f} (95% CI: [{bs['ci_lower']:.2f}, {bs['ci_upper']:.2f}])\")\n",
    "    print(f\"Standard Deviation: {bs['std']:.2f}\")\n",
    "    print(f\"Median: {bs['median']:.2f}\")\n",
    "    print(f\"Min: {bs['min']:.2f}\")\n",
    "    print(f\"Max: {bs['max']:.2f}\")\n",
    "    print(f\"Q1: {bs['q1']:.2f}\")\n",
    "    print(f\"Q3: {bs['q3']:.2f}\")\n",
    "\n",
    "def print_distribution(stats, dimension):\n",
    "    dist = stats['distribution']\n",
    "    print(f\"\\n{dimension} Value Distribution:\")\n",
    "    for val, count, pct in zip(dist['values'], dist['counts'], dist['percentages']):\n",
    "        print(f\"Value {val}: {count} occurrences ({pct:.1f}%)\")\n",
    "\n",
    "for dim_name, dim_data in dimensions.items():\n",
    "    print_basic_stats(dim_data['stats'], dim_name)\n",
    "    print_distribution(dim_data['stats'], dim_name)\n",
    "\n",
    "print(\"\\nPer-Claim Statistics:\")\n",
    "for i in range(1, 6):\n",
    "    stats = claim_stats[f'claim_{i}']\n",
    "    mean = stats['basic_stats']['mean']\n",
    "    ci_lower = stats['basic_stats']['ci_lower']\n",
    "    ci_upper = stats['basic_stats']['ci_upper']\n",
    "    print(f\"\\nClaim {i}: {mean:.2f} (95% CI: [{ci_lower:.2f}, {ci_upper:.2f}])\")\n",
    "    print_distribution(stats, f\"Claim {i}\")\n",
    "\n",
    "stats_data = {\n",
    "    'dimensions': dimensions,\n",
    "    'claims': claim_stats\n",
    "}"
   ],
   "id": "acf6cc4ce37503b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question Coverage Statistics:\n",
      "Mean: 3.91 (95% CI: [3.82, 3.99])\n",
      "Standard Deviation: 0.92\n",
      "Median: 4.00\n",
      "Min: 1.00\n",
      "Max: 5.00\n",
      "Q1: 3.00\n",
      "Q3: 5.00\n",
      "\n",
      "Question Coverage Value Distribution:\n",
      "Value 1: 1 occurrences (0.2%)\n",
      "Value 2: 38 occurrences (8.4%)\n",
      "Value 3: 94 occurrences (20.9%)\n",
      "Value 4: 185 occurrences (41.1%)\n",
      "Value 5: 132 occurrences (29.3%)\n",
      "\n",
      "Question Relevance Statistics:\n",
      "Mean: 3.84 (95% CI: [3.76, 3.93])\n",
      "Standard Deviation: 0.93\n",
      "Median: 4.00\n",
      "Min: 1.00\n",
      "Max: 5.00\n",
      "Q1: 3.00\n",
      "Q3: 5.00\n",
      "\n",
      "Question Relevance Value Distribution:\n",
      "Value 1: 1 occurrences (0.2%)\n",
      "Value 2: 42 occurrences (9.3%)\n",
      "Value 3: 104 occurrences (23.1%)\n",
      "Value 4: 183 occurrences (40.7%)\n",
      "Value 5: 120 occurrences (26.7%)\n",
      "\n",
      "Question Formulation Statistics:\n",
      "Mean: 3.74 (95% CI: [3.66, 3.83])\n",
      "Standard Deviation: 0.89\n",
      "Median: 4.00\n",
      "Min: 1.00\n",
      "Max: 5.00\n",
      "Q1: 3.00\n",
      "Q3: 4.00\n",
      "\n",
      "Question Formulation Value Distribution:\n",
      "Value 1: 4 occurrences (0.9%)\n",
      "Value 2: 37 occurrences (8.2%)\n",
      "Value 3: 117 occurrences (26.0%)\n",
      "Value 4: 204 occurrences (45.3%)\n",
      "Value 5: 88 occurrences (19.6%)\n",
      "\n",
      "Justification Explainability Statistics:\n",
      "Mean: 3.87 (95% CI: [3.79, 3.95])\n",
      "Standard Deviation: 0.88\n",
      "Median: 4.00\n",
      "Min: 1.00\n",
      "Max: 5.00\n",
      "Q1: 3.00\n",
      "Q3: 4.00\n",
      "\n",
      "Justification Explainability Value Distribution:\n",
      "Value 1: 1 occurrences (0.2%)\n",
      "Value 2: 35 occurrences (7.8%)\n",
      "Value 3: 98 occurrences (21.8%)\n",
      "Value 4: 204 occurrences (45.3%)\n",
      "Value 5: 112 occurrences (24.9%)\n",
      "\n",
      "Per-Claim Statistics:\n",
      "\n",
      "Claim 1: 3.91 (95% CI: [3.82, 4.00])\n",
      "\n",
      "Claim 1 Value Distribution:\n",
      "Value 2: 22 occurrences (6.1%)\n",
      "Value 3: 78 occurrences (21.7%)\n",
      "Value 4: 170 occurrences (47.2%)\n",
      "Value 5: 90 occurrences (25.0%)\n",
      "\n",
      "Claim 2: 3.82 (95% CI: [3.72, 3.91])\n",
      "\n",
      "Claim 2 Value Distribution:\n",
      "Value 1: 2 occurrences (0.6%)\n",
      "Value 2: 28 occurrences (7.8%)\n",
      "Value 3: 92 occurrences (25.6%)\n",
      "Value 4: 150 occurrences (41.7%)\n",
      "Value 5: 88 occurrences (24.4%)\n",
      "\n",
      "Claim 3: 3.91 (95% CI: [3.82, 4.01])\n",
      "\n",
      "Claim 3 Value Distribution:\n",
      "Value 1: 1 occurrences (0.3%)\n",
      "Value 2: 29 occurrences (8.1%)\n",
      "Value 3: 74 occurrences (20.6%)\n",
      "Value 4: 153 occurrences (42.5%)\n",
      "Value 5: 103 occurrences (28.6%)\n",
      "\n",
      "Claim 4: 3.71 (95% CI: [3.61, 3.80])\n",
      "\n",
      "Claim 4 Value Distribution:\n",
      "Value 2: 45 occurrences (12.5%)\n",
      "Value 3: 90 occurrences (25.0%)\n",
      "Value 4: 151 occurrences (41.9%)\n",
      "Value 5: 74 occurrences (20.6%)\n",
      "\n",
      "Claim 5: 3.86 (95% CI: [3.76, 3.96])\n",
      "\n",
      "Claim 5 Value Distribution:\n",
      "Value 1: 4 occurrences (1.1%)\n",
      "Value 2: 28 occurrences (7.8%)\n",
      "Value 3: 79 occurrences (21.9%)\n",
      "Value 4: 152 occurrences (42.2%)\n",
      "Value 5: 97 occurrences (26.9%)\n"
     ]
    }
   ],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T20:23:56.357835Z",
     "start_time": "2025-01-29T20:23:56.323232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# es statistics\n",
    "\n",
    "es_df = pd.read_csv(f\"{output_dir}/processed/es_process_phase_1.csv\")\n",
    "\n",
    "relevance_cols = [col for col in es_df.columns if 'explanations_relevance' in col]\n",
    "effectiveness_cols = [col for col in es_df.columns if 'explanations_effectiveness' in col]\n",
    "logical_cols = [col for col in es_df.columns if 'verdicts_logical_connection' in col]\n",
    "\n",
    "def get_comprehensive_stats(df, cols):\n",
    "   all_values = df[cols].values.ravel()\n",
    "   \n",
    "   mean = np.mean(all_values)\n",
    "   std = np.std(all_values)\n",
    "   n = len(all_values)\n",
    "   se = scipy.stats.sem(all_values)\n",
    "   ci = scipy.stats.t.interval(confidence=0.95, df=n-1, loc=mean, scale=se)\n",
    "   \n",
    "   value_counts = pd.Series(all_values).value_counts().sort_index()\n",
    "   percentages = (value_counts / len(all_values) * 100).round(2)\n",
    "   \n",
    "   return {\n",
    "       'basic_stats': {\n",
    "           'mean': mean,\n",
    "           'std': std,\n",
    "           'median': np.median(all_values),\n",
    "           'min': np.min(all_values),\n",
    "           'max': np.max(all_values),\n",
    "           'q1': np.percentile(all_values, 25),\n",
    "           'q3': np.percentile(all_values, 75),\n",
    "           'ci_lower': ci[0],\n",
    "           'ci_upper': ci[1]\n",
    "       },\n",
    "       'distribution': {\n",
    "           'values': value_counts.index.tolist(),\n",
    "           'counts': value_counts.tolist(),\n",
    "           'percentages': percentages.tolist()\n",
    "       },\n",
    "       'raw_values': all_values\n",
    "   }\n",
    "\n",
    "dimensions = {\n",
    "   'Explanations Relevance': {'cols': relevance_cols, 'stats': None},\n",
    "   'Explanations Effectiveness': {'cols': effectiveness_cols, 'stats': None},\n",
    "   'Verdicts Logical Connection': {'cols': logical_cols, 'stats': None}\n",
    "}\n",
    "\n",
    "for dim_name, dim_data in dimensions.items():\n",
    "   dim_data['stats'] = get_comprehensive_stats(es_df, dim_data['cols'])\n",
    "\n",
    "claim_stats = {}\n",
    "for i in range(1, 6):\n",
    "   claim_cols = [col for col in es_df.columns if f'claim{i}' in col]\n",
    "   claim_stats[f'claim_{i}'] = get_comprehensive_stats(es_df, claim_cols)\n",
    "\n",
    "def print_basic_stats(stats, dimension):\n",
    "   bs = stats['basic_stats']\n",
    "   print(f\"\\n{dimension} Statistics:\")\n",
    "   print(f\"Mean: {bs['mean']:.2f} (95% CI: [{bs['ci_lower']:.2f}, {bs['ci_upper']:.2f}])\")\n",
    "   print(f\"Standard Deviation: {bs['std']:.2f}\")\n",
    "   print(f\"Median: {bs['median']:.2f}\")\n",
    "   print(f\"Min: {bs['min']:.2f}\")\n",
    "   print(f\"Max: {bs['max']:.2f}\")\n",
    "   print(f\"Q1: {bs['q1']:.2f}\")\n",
    "   print(f\"Q3: {bs['q3']:.2f}\")\n",
    "\n",
    "def print_distribution(stats, dimension):\n",
    "   dist = stats['distribution']\n",
    "   print(f\"\\n{dimension} Value Distribution:\")\n",
    "   for val, count, pct in zip(dist['values'], dist['counts'], dist['percentages']):\n",
    "       print(f\"Value {val}: {count} occurrences ({pct:.1f}%)\")\n",
    "\n",
    "for dim_name, dim_data in dimensions.items():\n",
    "   print_basic_stats(dim_data['stats'], dim_name)\n",
    "   print_distribution(dim_data['stats'], dim_name)\n",
    "\n",
    "print(\"\\nPer-Claim Statistics:\")\n",
    "for i in range(1, 6):\n",
    "   stats = claim_stats[f'claim_{i}']\n",
    "   mean = stats['basic_stats']['mean']\n",
    "   ci_lower = stats['basic_stats']['ci_lower']\n",
    "   ci_upper = stats['basic_stats']['ci_upper']\n",
    "   print(f\"\\nClaim {i}: {mean:.2f} (95% CI: [{ci_lower:.2f}, {ci_upper:.2f}])\")\n",
    "   print_distribution(stats, f\"Claim {i}\")\n",
    "\n",
    "stats_data = {\n",
    "   'dimensions': dimensions,\n",
    "   'claims': claim_stats\n",
    "}"
   ],
   "id": "7033e05f30603e91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explanations Relevance Statistics:\n",
      "Mean: 3.66 (95% CI: [3.58, 3.75])\n",
      "Standard Deviation: 0.93\n",
      "Median: 4.00\n",
      "Min: 1.00\n",
      "Max: 5.00\n",
      "Q1: 3.00\n",
      "Q3: 4.00\n",
      "\n",
      "Explanations Relevance Value Distribution:\n",
      "Value 1: 5 occurrences (1.1%)\n",
      "Value 2: 48 occurrences (10.7%)\n",
      "Value 3: 123 occurrences (27.3%)\n",
      "Value 4: 191 occurrences (42.4%)\n",
      "Value 5: 83 occurrences (18.4%)\n",
      "\n",
      "Explanations Effectiveness Statistics:\n",
      "Mean: 3.61 (95% CI: [3.52, 3.70])\n",
      "Standard Deviation: 0.96\n",
      "Median: 4.00\n",
      "Min: 1.00\n",
      "Max: 5.00\n",
      "Q1: 3.00\n",
      "Q3: 4.00\n",
      "\n",
      "Explanations Effectiveness Value Distribution:\n",
      "Value 1: 6 occurrences (1.3%)\n",
      "Value 2: 57 occurrences (12.7%)\n",
      "Value 3: 122 occurrences (27.1%)\n",
      "Value 4: 185 occurrences (41.1%)\n",
      "Value 5: 80 occurrences (17.8%)\n",
      "\n",
      "Verdicts Logical Connection Statistics:\n",
      "Mean: 3.67 (95% CI: [3.58, 3.76])\n",
      "Standard Deviation: 0.95\n",
      "Median: 4.00\n",
      "Min: 1.00\n",
      "Max: 5.00\n",
      "Q1: 3.00\n",
      "Q3: 4.00\n",
      "\n",
      "Verdicts Logical Connection Value Distribution:\n",
      "Value 1: 3 occurrences (0.7%)\n",
      "Value 2: 53 occurrences (11.8%)\n",
      "Value 3: 124 occurrences (27.6%)\n",
      "Value 4: 180 occurrences (40.0%)\n",
      "Value 5: 90 occurrences (20.0%)\n",
      "\n",
      "Per-Claim Statistics:\n",
      "\n",
      "Claim 1: 3.74 (95% CI: [3.63, 3.85])\n",
      "\n",
      "Claim 1 Value Distribution:\n",
      "Value 1: 1 occurrences (0.4%)\n",
      "Value 2: 31 occurrences (11.5%)\n",
      "Value 3: 62 occurrences (23.0%)\n",
      "Value 4: 119 occurrences (44.1%)\n",
      "Value 5: 57 occurrences (21.1%)\n",
      "\n",
      "Claim 2: 3.64 (95% CI: [3.52, 3.76])\n",
      "\n",
      "Claim 2 Value Distribution:\n",
      "Value 1: 5 occurrences (1.9%)\n",
      "Value 2: 31 occurrences (11.5%)\n",
      "Value 3: 75 occurrences (27.8%)\n",
      "Value 4: 104 occurrences (38.5%)\n",
      "Value 5: 55 occurrences (20.4%)\n",
      "\n",
      "Claim 3: 3.77 (95% CI: [3.66, 3.87])\n",
      "\n",
      "Claim 3 Value Distribution:\n",
      "Value 2: 26 occurrences (9.6%)\n",
      "Value 3: 69 occurrences (25.6%)\n",
      "Value 4: 117 occurrences (43.3%)\n",
      "Value 5: 58 occurrences (21.5%)\n",
      "\n",
      "Claim 4: 3.39 (95% CI: [3.26, 3.51])\n",
      "\n",
      "Claim 4 Value Distribution:\n",
      "Value 1: 6 occurrences (2.2%)\n",
      "Value 2: 50 occurrences (18.5%)\n",
      "Value 3: 84 occurrences (31.1%)\n",
      "Value 4: 94 occurrences (34.8%)\n",
      "Value 5: 36 occurrences (13.3%)\n",
      "\n",
      "Claim 5: 3.71 (95% CI: [3.61, 3.81])\n",
      "\n",
      "Claim 5 Value Distribution:\n",
      "Value 1: 2 occurrences (0.7%)\n",
      "Value 2: 20 occurrences (7.4%)\n",
      "Value 3: 79 occurrences (29.3%)\n",
      "Value 4: 122 occurrences (45.2%)\n",
      "Value 5: 47 occurrences (17.4%)\n"
     ]
    }
   ],
   "execution_count": 162
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T20:27:19.016662Z",
     "start_time": "2025-01-29T20:27:18.997144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# fc statistics\n",
    "\n",
    "fc_df = pd.read_csv(f\"{output_dir}/processed/fc_process_phase_1.csv\")\n",
    "\n",
    "coverage_cols = [col for col in fc_df.columns if 'summary_coverage' in col]\n",
    "support_cols = [col for col in fc_df.columns if 'summary_verdict_support' in col]\n",
    "alignment_cols = [col for col in fc_df.columns if 'summaries_allignment' in col]\n",
    "choice_cols = [col for col in fc_df.columns if 'summary_choice' in col]\n",
    "\n",
    "def get_comprehensive_stats(df, cols, scale_max):\n",
    "    all_values = df[cols].values.ravel()\n",
    "    \n",
    "    mean = np.mean(all_values)\n",
    "    std = np.std(all_values)\n",
    "    n = len(all_values)\n",
    "    se = scipy.stats.sem(all_values)\n",
    "    ci = scipy.stats.t.interval(confidence=0.95, df=n-1, loc=mean, scale=se)\n",
    "    \n",
    "    value_counts = pd.Series(all_values).value_counts().sort_index()\n",
    "    percentages = (value_counts / len(all_values) * 100).round(2)\n",
    "    \n",
    "    return {\n",
    "        'basic_stats': {\n",
    "            'mean': mean,\n",
    "            'std': std,\n",
    "            'median': np.median(all_values),\n",
    "            'min': np.min(all_values),\n",
    "            'max': np.max(all_values),\n",
    "            'q1': np.percentile(all_values, 25),\n",
    "            'q3': np.percentile(all_values, 75),\n",
    "            'ci_lower': ci[0],\n",
    "            'ci_upper': ci[1],\n",
    "            'mean_percentage': (mean/scale_max)*100\n",
    "        },\n",
    "        'distribution': {\n",
    "            'values': value_counts.index.tolist(),\n",
    "            'counts': value_counts.tolist(),\n",
    "            'percentages': percentages.tolist()\n",
    "        },\n",
    "        'raw_values': all_values\n",
    "    }\n",
    "\n",
    "dimensions = {\n",
    "    'Summary Coverage': {'cols': coverage_cols, 'scale_max': 5, 'stats': None},\n",
    "    'Summary Verdict Support': {'cols': support_cols, 'scale_max': 5, 'stats': None},\n",
    "    'Summaries Alignment': {'cols': alignment_cols, 'scale_max': 4, 'stats': None},\n",
    "    'Summary Choice': {'cols': choice_cols, 'scale_max': 5, 'stats': None}\n",
    "}\n",
    "\n",
    "for dim_name, dim_data in dimensions.items():\n",
    "    dim_data['stats'] = get_comprehensive_stats(fc_df, dim_data['cols'], dim_data['scale_max'])\n",
    "\n",
    "def print_basic_stats(stats, dimension, scale_max):\n",
    "    bs = stats['basic_stats']\n",
    "    print(f\"\\n{dimension} Statistics (Scale 1-{scale_max}):\")\n",
    "    print(f\"Mean: {bs['mean']:.2f} (95% CI: [{bs['ci_lower']:.2f}, {bs['ci_upper']:.2f}])\")\n",
    "    print(f\"Standard Deviation: {bs['std']:.2f}\")\n",
    "    print(f\"Median: {bs['median']:.2f}\")\n",
    "    print(f\"Min: {bs['min']:.2f}\")\n",
    "    print(f\"Max: {bs['max']:.2f}\")\n",
    "    print(f\"Q1: {bs['q1']:.2f}\")\n",
    "    print(f\"Q3: {bs['q3']:.2f}\")\n",
    "    print(f\"Mean as % of maximum: {bs['mean_percentage']:.1f}%\")\n",
    "\n",
    "def print_distribution(stats, dimension):\n",
    "    dist = stats['distribution']\n",
    "    print(f\"\\n{dimension} Value Distribution:\")\n",
    "    for val, count, pct in zip(dist['values'], dist['counts'], dist['percentages']):\n",
    "        print(f\"Value {val}: {count} occurrences ({pct:.1f}%)\")\n",
    "\n",
    "for dim_name, dim_data in dimensions.items():\n",
    "    print_basic_stats(dim_data['stats'], dim_name, dim_data['scale_max'])\n",
    "    print_distribution(dim_data['stats'], dim_name)\n",
    "\n",
    "print(\"\\nPer-Claim Analysis (Separated by Scale):\")\n",
    "\n",
    "scale5_dimensions = ['Summary Coverage', 'Summary Verdict Support', 'Summary Choice']\n",
    "print(\"\\nAnalysis for 1-5 scale metrics:\")\n",
    "for i in range(1, 6):\n",
    "    claim_cols = []\n",
    "    for dim in scale5_dimensions:\n",
    "        cols = [col for col in dimensions[dim]['cols'] if f'claim{i}' in col]\n",
    "        claim_cols.extend(cols)\n",
    "    if claim_cols:\n",
    "        stats = get_comprehensive_stats(fc_df, claim_cols, 5)\n",
    "        print(f\"\\nClaim {i} (1-5 scale metrics):\")\n",
    "        print(f\"Mean: {stats['basic_stats']['mean']:.2f} (95% CI: [{stats['basic_stats']['ci_lower']:.2f}, {stats['basic_stats']['ci_upper']:.2f}])\")\n",
    "        print_distribution(stats, f\"Claim {i}\")\n",
    "\n",
    "print(\"\\nAnalysis for 1-4 scale metric (Summaries Alignment):\")\n",
    "for i in range(1, 6):\n",
    "    claim_cols = [col for col in alignment_cols if f'claim{i}' in col]\n",
    "    if claim_cols:\n",
    "        stats = get_comprehensive_stats(fc_df, claim_cols, 4)\n",
    "        print(f\"\\nClaim {i} Alignment:\")\n",
    "        print(f\"Mean: {stats['basic_stats']['mean']:.2f} (95% CI: [{stats['basic_stats']['ci_lower']:.2f}, {stats['basic_stats']['ci_upper']:.2f}])\")\n",
    "        print_distribution(stats, f\"Claim {i}\")"
   ],
   "id": "9526d2e28bee9ac1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Coverage Statistics (Scale 1-5):\n",
      "Mean: 3.58 (95% CI: [3.49, 3.67])\n",
      "Standard Deviation: 0.96\n",
      "Median: 4.00\n",
      "Min: 1.00\n",
      "Max: 5.00\n",
      "Q1: 3.00\n",
      "Q3: 4.00\n",
      "Mean as % of maximum: 71.6%\n",
      "\n",
      "Summary Coverage Value Distribution:\n",
      "Value 1: 3 occurrences (0.7%)\n",
      "Value 2: 62 occurrences (13.8%)\n",
      "Value 3: 138 occurrences (30.7%)\n",
      "Value 4: 164 occurrences (36.4%)\n",
      "Value 5: 83 occurrences (18.4%)\n",
      "\n",
      "Summary Verdict Support Statistics (Scale 1-5):\n",
      "Mean: 3.57 (95% CI: [3.48, 3.66])\n",
      "Standard Deviation: 0.95\n",
      "Median: 4.00\n",
      "Min: 1.00\n",
      "Max: 5.00\n",
      "Q1: 3.00\n",
      "Q3: 4.00\n",
      "Mean as % of maximum: 71.4%\n",
      "\n",
      "Summary Verdict Support Value Distribution:\n",
      "Value 1: 6 occurrences (1.3%)\n",
      "Value 2: 54 occurrences (12.0%)\n",
      "Value 3: 144 occurrences (32.0%)\n",
      "Value 4: 169 occurrences (37.6%)\n",
      "Value 5: 77 occurrences (17.1%)\n",
      "\n",
      "Summaries Alignment Statistics (Scale 1-4):\n",
      "Mean: 2.81 (95% CI: [2.73, 2.90])\n",
      "Standard Deviation: 0.91\n",
      "Median: 3.00\n",
      "Min: 1.00\n",
      "Max: 4.00\n",
      "Q1: 2.00\n",
      "Q3: 3.00\n",
      "Mean as % of maximum: 70.3%\n",
      "\n",
      "Summaries Alignment Value Distribution:\n",
      "Value 1: 45 occurrences (10.0%)\n",
      "Value 2: 101 occurrences (22.4%)\n",
      "Value 3: 197 occurrences (43.8%)\n",
      "Value 4: 107 occurrences (23.8%)\n",
      "\n",
      "Summary Choice Statistics (Scale 1-5):\n",
      "Mean: 3.61 (95% CI: [3.50, 3.72])\n",
      "Standard Deviation: 1.21\n",
      "Median: 4.00\n",
      "Min: 1.00\n",
      "Max: 5.00\n",
      "Q1: 3.00\n",
      "Q3: 5.00\n",
      "Mean as % of maximum: 72.2%\n",
      "\n",
      "Summary Choice Value Distribution:\n",
      "Value 1: 24 occurrences (5.3%)\n",
      "Value 2: 68 occurrences (15.1%)\n",
      "Value 3: 102 occurrences (22.7%)\n",
      "Value 4: 122 occurrences (27.1%)\n",
      "Value 5: 134 occurrences (29.8%)\n",
      "\n",
      "Per-Claim Analysis (Separated by Scale):\n",
      "\n",
      "Analysis for 1-5 scale metrics:\n",
      "\n",
      "Claim 1 (1-5 scale metrics):\n",
      "Mean: 3.73 (95% CI: [3.60, 3.85])\n",
      "\n",
      "Claim 1 Value Distribution:\n",
      "Value 1: 3 occurrences (1.1%)\n",
      "Value 2: 34 occurrences (12.6%)\n",
      "Value 3: 67 occurrences (24.8%)\n",
      "Value 4: 96 occurrences (35.6%)\n",
      "Value 5: 70 occurrences (25.9%)\n",
      "\n",
      "Claim 2 (1-5 scale metrics):\n",
      "Mean: 3.45 (95% CI: [3.33, 3.58])\n",
      "\n",
      "Claim 2 Value Distribution:\n",
      "Value 1: 8 occurrences (3.0%)\n",
      "Value 2: 41 occurrences (15.2%)\n",
      "Value 3: 87 occurrences (32.2%)\n",
      "Value 4: 89 occurrences (33.0%)\n",
      "Value 5: 45 occurrences (16.7%)\n",
      "\n",
      "Claim 3 (1-5 scale metrics):\n",
      "Mean: 3.72 (95% CI: [3.59, 3.86])\n",
      "\n",
      "Claim 3 Value Distribution:\n",
      "Value 1: 13 occurrences (4.8%)\n",
      "Value 2: 22 occurrences (8.2%)\n",
      "Value 3: 73 occurrences (27.0%)\n",
      "Value 4: 81 occurrences (30.0%)\n",
      "Value 5: 81 occurrences (30.0%)\n",
      "\n",
      "Claim 4 (1-5 scale metrics):\n",
      "Mean: 3.40 (95% CI: [3.28, 3.53])\n",
      "\n",
      "Claim 4 Value Distribution:\n",
      "Value 1: 4 occurrences (1.5%)\n",
      "Value 2: 55 occurrences (20.4%)\n",
      "Value 3: 79 occurrences (29.3%)\n",
      "Value 4: 92 occurrences (34.1%)\n",
      "Value 5: 40 occurrences (14.8%)\n",
      "\n",
      "Claim 5 (1-5 scale metrics):\n",
      "Mean: 3.63 (95% CI: [3.51, 3.75])\n",
      "\n",
      "Claim 5 Value Distribution:\n",
      "Value 1: 5 occurrences (1.9%)\n",
      "Value 2: 32 occurrences (11.8%)\n",
      "Value 3: 78 occurrences (28.9%)\n",
      "Value 4: 97 occurrences (35.9%)\n",
      "Value 5: 58 occurrences (21.5%)\n",
      "\n",
      "Analysis for 1-4 scale metric (Summaries Alignment):\n",
      "\n",
      "Claim 1 Alignment:\n",
      "Mean: 2.83 (95% CI: [2.66, 3.01])\n",
      "\n",
      "Claim 1 Value Distribution:\n",
      "Value 1: 6 occurrences (6.7%)\n",
      "Value 2: 21 occurrences (23.3%)\n",
      "Value 3: 45 occurrences (50.0%)\n",
      "Value 4: 18 occurrences (20.0%)\n",
      "\n",
      "Claim 2 Alignment:\n",
      "Mean: 2.59 (95% CI: [2.41, 2.76])\n",
      "\n",
      "Claim 2 Value Distribution:\n",
      "Value 1: 8 occurrences (8.9%)\n",
      "Value 2: 33 occurrences (36.7%)\n",
      "Value 3: 37 occurrences (41.1%)\n",
      "Value 4: 12 occurrences (13.3%)\n",
      "\n",
      "Claim 3 Alignment:\n",
      "Mean: 3.22 (95% CI: [3.04, 3.40])\n",
      "\n",
      "Claim 3 Value Distribution:\n",
      "Value 1: 6 occurrences (6.7%)\n",
      "Value 2: 8 occurrences (8.9%)\n",
      "Value 3: 36 occurrences (40.0%)\n",
      "Value 4: 40 occurrences (44.4%)\n",
      "\n",
      "Claim 4 Alignment:\n",
      "Mean: 2.67 (95% CI: [2.45, 2.88])\n",
      "\n",
      "Claim 4 Value Distribution:\n",
      "Value 1: 16 occurrences (17.8%)\n",
      "Value 2: 18 occurrences (20.0%)\n",
      "Value 3: 36 occurrences (40.0%)\n",
      "Value 4: 20 occurrences (22.2%)\n",
      "\n",
      "Claim 5 Alignment:\n",
      "Mean: 2.76 (95% CI: [2.57, 2.94])\n",
      "\n",
      "Claim 5 Value Distribution:\n",
      "Value 1: 9 occurrences (10.0%)\n",
      "Value 2: 21 occurrences (23.3%)\n",
      "Value 3: 43 occurrences (47.8%)\n",
      "Value 4: 17 occurrences (18.9%)\n"
     ]
    }
   ],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T18:33:10.522674Z",
     "start_time": "2025-01-29T18:33:10.503248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# processing participants data\n",
    "\n",
    "df = pd.read_csv(f\"{output_dir}/participants_data.csv\")\n",
    "participants_processed = df.copy()\n",
    "\n",
    "participants_processed = participants_processed.drop('email', axis=1)\n",
    "\n",
    "participants_processed['english_level'] = participants_processed['english_level'].str.extract(r'\\((.*?)\\)', expand=False)\n",
    "participants_processed['political_orientation'] = participants_processed['political_orientation'].str.extract(r'\\((.*?)\\)', expand=False)\n",
    "\n",
    "output_path = os.path.join(processed_dir, 'participants_data_phase_1.csv')\n",
    "participants_processed.to_csv(output_path, index=False)"
   ],
   "id": "510e7788b9a07f62",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T18:37:21.247026Z",
     "start_time": "2025-01-29T18:37:21.216131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# demographics data information\n",
    "\n",
    "participants = pd.read_csv(f\"{output_dir}/processed/participants_data_phase_1.csv\")\n",
    "\n",
    "print(\"Demographics Distribution:\\n\")\n",
    "print(\"Age Groups:\")\n",
    "print(participants['age_group'].value_counts(normalize=True) * 100)\n",
    "print(\"\\nEducation Levels:\")\n",
    "print(participants['education_level'].value_counts(normalize=True) * 100)\n",
    "print(\"\\nEnglish Proficiency:\")\n",
    "print(participants['english_level'].value_counts(normalize=True) * 100)\n",
    "print(\"\\nPolitical Orientation:\")\n",
    "print(participants['political_orientation'].value_counts(normalize=True) * 100)\n",
    "print(\"\\nFact-checking Experience (years):\")\n",
    "print(participants['fc_years_of_experience'].value_counts(normalize=True) * 100)"
   ],
   "id": "eaa2d368a4abd27f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics Distribution:\n",
      "\n",
      "Age Groups:\n",
      "age_group\n",
      "26-35 years old      37.777778\n",
      "36-50 years old      28.888889\n",
      "18-25 years old      27.777778\n",
      "Over 50 years old     5.555556\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Education Levels:\n",
      "education_level\n",
      "Bachelor's degree                    51.111111\n",
      "Master's degree                      37.777778\n",
      "High school diploma or equivalent     5.555556\n",
      "Doctoral degree (PhD)                 3.333333\n",
      "I prefer not to answer                2.222222\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "English Proficiency:\n",
      "english_level\n",
      "Native speaker          64.044944\n",
      "High proficiency        28.089888\n",
      "Moderate proficiency     6.741573\n",
      "Basic comprehension      1.123596\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Political Orientation:\n",
      "political_orientation\n",
      "Very Liberal               38.372093\n",
      "Moderately Liberal         32.558140\n",
      "Moderate                   18.604651\n",
      "Moderately Conservative     9.302326\n",
      "Very Conservative           1.162791\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fact-checking Experience (years):\n",
      "fc_years_of_experience\n",
      "2-5 years                                40.000000\n",
      "More than 10 years                       24.444444\n",
      "Less than 2 years                        14.444444\n",
      "5-10 years                               13.333333\n",
      "No dedicated fact-checking experience     7.777778\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T21:17:57.562352Z",
     "start_time": "2025-01-29T21:17:57.338616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# processed averages per participant\n",
    "\n",
    "cd_df = pd.read_csv(f\"{output_dir}/processed/cd_process_phase_1.csv\")\n",
    "es_df = pd.read_csv(f\"{output_dir}/processed/es_process_phase_1.csv\")\n",
    "fc_df = pd.read_csv(f\"{output_dir}/processed/fc_process_phase_1.csv\")\n",
    "proc_df = pd.read_csv(f\"{output_dir}/processed/process_evaluation_phase_1.csv\")\n",
    "participants_df = pd.read_csv(f\"{output_dir}/processed/participants_data_phase_1.csv\")\n",
    "\n",
    "participant_averages = {}\n",
    "participant_ids = cd_df['prolific_id'].unique()\n",
    "\n",
    "for pid in participant_ids:\n",
    "    participant_averages[pid] = {}\n",
    "    \n",
    "    cd_participant = cd_df[cd_df['prolific_id'] == pid]\n",
    "    participant_averages[pid].update({\n",
    "        'cd_overall': cd_participant.iloc[:, 1:].mean().mean(),  # Skip prolific_id column\n",
    "        'cd_coverage': cd_participant[[col for col in cd_participant.columns if 'coverage' in col]].mean().mean(),\n",
    "        'cd_relevance': cd_participant[[col for col in cd_participant.columns if 'relevance' in col]].mean().mean(),\n",
    "        'cd_formulation': cd_participant[[col for col in cd_participant.columns if 'formulation' in col]].mean().mean(),\n",
    "        'cd_explainability': cd_participant[[col for col in cd_participant.columns if 'explainability' in col]].mean().mean()\n",
    "    })\n",
    "    \n",
    "    # ES averages\n",
    "    es_participant = es_df[es_df['prolific_id'] == pid]\n",
    "    participant_averages[pid].update({\n",
    "        'es_overall': es_participant.iloc[:, 1:].mean().mean(),  # Skip prolific_id column\n",
    "        'es_relevance': es_participant[[col for col in es_participant.columns if 'relevance' in col]].mean().mean(),\n",
    "        'es_effectiveness': es_participant[[col for col in es_participant.columns if 'effectiveness' in col]].mean().mean(),\n",
    "        'es_logical_connection': es_participant[[col for col in es_participant.columns if 'logical_connection' in col]].mean().mean()\n",
    "    })\n",
    "    \n",
    "    fc_participant = fc_df[fc_df['prolific_id'] == pid]\n",
    "    scale5_cols = ([col for col in fc_participant.columns if 'coverage' in col] + \n",
    "                   [col for col in fc_participant.columns if 'support' in col] + \n",
    "                   [col for col in fc_participant.columns if 'choice' in col])\n",
    "    participant_averages[pid].update({\n",
    "        'fc_scale5_overall': fc_participant[scale5_cols].mean().mean(),\n",
    "        'fc_coverage': fc_participant[[col for col in fc_participant.columns if 'coverage' in col]].mean().mean(),\n",
    "        'fc_support': fc_participant[[col for col in fc_participant.columns if 'support' in col]].mean().mean(),\n",
    "        'fc_choice': fc_participant[[col for col in fc_participant.columns if 'choice' in col]].mean().mean(),\n",
    "        'fc_alignment': fc_participant[[col for col in fc_participant.columns if 'allignment' in col]].mean().mean()\n",
    "    })\n",
    "    \n",
    "    proc_participant = proc_df[proc_df['prolific_id'] == pid]\n",
    "    scale5_cols = ['process_explainability', 'process_transparency', 'sources_transparency', 'process_level_of_trust']\n",
    "    participant_averages[pid].update({\n",
    "        'proc_scale5_overall': proc_participant[scale5_cols].mean().mean(),\n",
    "        'proc_explainability': proc_participant['process_explainability'].values[0],\n",
    "        'proc_transparency': proc_participant['process_transparency'].values[0],\n",
    "        'proc_sources_transparency': proc_participant['sources_transparency'].values[0],\n",
    "        'proc_trust': proc_participant['process_level_of_trust'].values[0],\n",
    "        'proc_credibility': proc_participant['process_credibility'].values[0]\n",
    "    })\n",
    "    \n",
    "    participant_demo = participants_df[participants_df['prolific_id'] == pid]\n",
    "    participant_averages[pid].update({\n",
    "        'age_group': participant_demo['age_group'].values[0],\n",
    "        'english_level': participant_demo['english_level'].values[0],\n",
    "        'education_level': participant_demo['education_level'].values[0],\n",
    "        'political_orientation': participant_demo['political_orientation'].values[0],\n",
    "        'fc_years_of_experience': participant_demo['fc_years_of_experience'].values[0]\n",
    "    })\n",
    "\n",
    "participant_averages_df = pd.DataFrame.from_dict(participant_averages, orient='index')\n",
    "participant_averages_df.index.name = 'prolific_id'\n",
    "output_path = f\"{output_dir}/processed/participants_averages_phase_1.csv\"\n",
    "participant_averages_df.to_csv(output_path)"
   ],
   "id": "623d65a3711be69e",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T21:34:27.398320Z",
     "start_time": "2025-01-29T21:34:27.275665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "df = pd.read_csv(f\"{output_dir}/processed/participants_averages_phase_1.csv\")\n",
    "\n",
    "evaluation_metrics = {\n",
    "    'Claim Decomposition': ['cd_overall', 'cd_coverage', 'cd_relevance', 'cd_formulation', 'cd_explainability'],\n",
    "    'Evidence Synthesis': ['es_overall', 'es_relevance', 'es_effectiveness', 'es_logical_connection'],\n",
    "    'Final Conclusion': ['fc_scale5_overall', 'fc_coverage', 'fc_support', 'fc_choice', 'fc_alignment'],\n",
    "    'Process Evaluation': ['proc_scale5_overall', 'proc_explainability', 'proc_transparency', \n",
    "                          'proc_sources_transparency', 'proc_trust']\n",
    "}\n",
    "\n",
    "def run_statistical_tests(group_column, value_columns, df):\n",
    "    results = []\n",
    "    \n",
    "    for col in value_columns:\n",
    "        valid_data = df[[group_column, col]].dropna()\n",
    "        \n",
    "        if len(valid_data) > 0:\n",
    "            # Kruskal-Wallis H-test\n",
    "            groups = [group for _, group in valid_data.groupby(group_column)[col]]\n",
    "            h_stat, p_value = stats.kruskal(*groups)\n",
    "            \n",
    "            # Get mean values and counts for each group\n",
    "            group_stats = valid_data.groupby(group_column)[col].agg(['mean', 'count'])\n",
    "            \n",
    "            results.append({\n",
    "                'Metric': col,\n",
    "                'H-statistic': h_stat,\n",
    "                'p-value': p_value,\n",
    "                'Group_Stats': group_stats\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def analyze_demographic(demographic_col, df, metrics_dict):\n",
    "    print(f\"\\nAnalysis for {demographic_col}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Print distribution of demographic groups\n",
    "    print(\"\\nGroup Distribution:\")\n",
    "    group_dist = df[demographic_col].value_counts()\n",
    "    for group, count in group_dist.items():\n",
    "        print(f\"{group}: {count} participants ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    for metric_group, metrics in metrics_dict.items():\n",
    "        results = run_statistical_tests(demographic_col, metrics, df)\n",
    "        \n",
    "        # Print significant results (p < 0.05)\n",
    "        sig_results = results[results['p-value'] < 0.05]\n",
    "        if not sig_results.empty:\n",
    "            print(f\"\\n{metric_group} Metrics:\")\n",
    "            print(\"\\nSignificant differences found:\")\n",
    "            for _, row in sig_results.iterrows():\n",
    "                print(f\"\\n{row['Metric']}:\")\n",
    "                print(f\"H-statistic: {row['H-statistic']:.2f}\")\n",
    "                print(f\"p-value: {row['p-value']:.4f}\")\n",
    "                \n",
    "                # Print group statistics\n",
    "                stats_df = row['Group_Stats']\n",
    "                print(\"\\nGroup statistics:\")\n",
    "                for group in stats_df.index:\n",
    "                    mean = stats_df.loc[group, 'mean']\n",
    "                    count = stats_df.loc[group, 'count']\n",
    "                    print(f\"  {group}: {mean:.2f} (n={count})\")\n",
    "                \n",
    "                # Only make comparisons for groups with sufficient sample size (e.g., n > 5)\n",
    "                valid_groups = stats_df[stats_df['count'] > 5]\n",
    "                if len(valid_groups) >= 2:\n",
    "                    print(\"\\nComparison between groups with sufficient sample size:\")\n",
    "                    print(f\"Range of scores: {valid_groups['mean'].min():.2f} to {valid_groups['mean'].max():.2f}\")\n",
    "                    \n",
    "        else:\n",
    "            print(f\"\\n{metric_group} Metrics:\")\n",
    "            print(\"No significant differences found between groups\")\n",
    "\n",
    "# Run analysis for each demographic factor including fc_years_of_experience\n",
    "demographics = ['age_group', 'education_level', 'english_level', \n",
    "                'political_orientation', 'fc_years_of_experience']\n",
    "for demo in demographics:\n",
    "    analyze_demographic(demo, df, evaluation_metrics)"
   ],
   "id": "83e8c1ca7c1a5543",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis for age_group:\n",
      "--------------------------------------------------\n",
      "\n",
      "Group Distribution:\n",
      "26-35 years old: 34 participants (37.8%)\n",
      "36-50 years old: 26 participants (28.9%)\n",
      "18-25 years old: 25 participants (27.8%)\n",
      "Over 50 years old: 5 participants (5.6%)\n",
      "\n",
      "Claim Decomposition Metrics:\n",
      "\n",
      "Significant differences found:\n",
      "\n",
      "cd_formulation:\n",
      "H-statistic: 8.16\n",
      "p-value: 0.0427\n",
      "\n",
      "Group statistics:\n",
      "  18-25 years old: 3.77 (n=25)\n",
      "  26-35 years old: 3.55 (n=34)\n",
      "  36-50 years old: 3.98 (n=26)\n",
      "  Over 50 years old: 3.72 (n=5)\n",
      "\n",
      "Comparison between groups with sufficient sample size:\n",
      "Range of scores: 3.55 to 3.98\n",
      "\n",
      "Evidence Synthesis Metrics:\n",
      "No significant differences found between groups\n",
      "\n",
      "Final Conclusion Metrics:\n",
      "No significant differences found between groups\n",
      "\n",
      "Process Evaluation Metrics:\n",
      "No significant differences found between groups\n",
      "\n",
      "Analysis for education_level:\n",
      "--------------------------------------------------\n",
      "\n",
      "Group Distribution:\n",
      "Bachelor's degree: 46 participants (51.1%)\n",
      "Master's degree: 34 participants (37.8%)\n",
      "High school diploma or equivalent: 5 participants (5.6%)\n",
      "Doctoral degree (PhD): 3 participants (3.3%)\n",
      "I prefer not to answer: 2 participants (2.2%)\n",
      "\n",
      "Claim Decomposition Metrics:\n",
      "No significant differences found between groups\n",
      "\n",
      "Evidence Synthesis Metrics:\n",
      "\n",
      "Significant differences found:\n",
      "\n",
      "es_overall:\n",
      "H-statistic: 12.21\n",
      "p-value: 0.0159\n",
      "\n",
      "Group statistics:\n",
      "  Bachelor's degree: 3.45 (n=46)\n",
      "  Doctoral degree (PhD): 4.13 (n=3)\n",
      "  High school diploma or equivalent: 3.95 (n=5)\n",
      "  I prefer not to answer: 3.30 (n=2)\n",
      "  Master's degree: 3.85 (n=34)\n",
      "\n",
      "Comparison between groups with sufficient sample size:\n",
      "Range of scores: 3.45 to 3.85\n",
      "\n",
      "es_relevance:\n",
      "H-statistic: 11.01\n",
      "p-value: 0.0265\n",
      "\n",
      "Group statistics:\n",
      "  Bachelor's degree: 3.46 (n=46)\n",
      "  Doctoral degree (PhD): 4.27 (n=3)\n",
      "  High school diploma or equivalent: 4.00 (n=5)\n",
      "  I prefer not to answer: 3.50 (n=2)\n",
      "  Master's degree: 3.85 (n=34)\n",
      "\n",
      "Comparison between groups with sufficient sample size:\n",
      "Range of scores: 3.46 to 3.85\n",
      "\n",
      "es_effectiveness:\n",
      "H-statistic: 10.04\n",
      "p-value: 0.0398\n",
      "\n",
      "Group statistics:\n",
      "  Bachelor's degree: 3.43 (n=46)\n",
      "  Doctoral degree (PhD): 3.93 (n=3)\n",
      "  High school diploma or equivalent: 3.92 (n=5)\n",
      "  I prefer not to answer: 3.30 (n=2)\n",
      "  Master's degree: 3.81 (n=34)\n",
      "\n",
      "Comparison between groups with sufficient sample size:\n",
      "Range of scores: 3.43 to 3.81\n",
      "\n",
      "es_logical_connection:\n",
      "H-statistic: 12.91\n",
      "p-value: 0.0117\n",
      "\n",
      "Group statistics:\n",
      "  Bachelor's degree: 3.46 (n=46)\n",
      "  Doctoral degree (PhD): 4.20 (n=3)\n",
      "  High school diploma or equivalent: 3.92 (n=5)\n",
      "  I prefer not to answer: 3.10 (n=2)\n",
      "  Master's degree: 3.91 (n=34)\n",
      "\n",
      "Comparison between groups with sufficient sample size:\n",
      "Range of scores: 3.46 to 3.91\n",
      "\n",
      "Final Conclusion Metrics:\n",
      "No significant differences found between groups\n",
      "\n",
      "Process Evaluation Metrics:\n",
      "\n",
      "Significant differences found:\n",
      "\n",
      "proc_sources_transparency:\n",
      "H-statistic: 10.65\n",
      "p-value: 0.0308\n",
      "\n",
      "Group statistics:\n",
      "  Bachelor's degree: 3.39 (n=46)\n",
      "  Doctoral degree (PhD): 3.67 (n=3)\n",
      "  High school diploma or equivalent: 2.60 (n=5)\n",
      "  I prefer not to answer: 4.50 (n=2)\n",
      "  Master's degree: 3.03 (n=34)\n",
      "\n",
      "Comparison between groups with sufficient sample size:\n",
      "Range of scores: 3.03 to 3.39\n",
      "\n",
      "Analysis for english_level:\n",
      "--------------------------------------------------\n",
      "\n",
      "Group Distribution:\n",
      "Native speaker: 57 participants (63.3%)\n",
      "High proficiency: 25 participants (27.8%)\n",
      "Moderate proficiency: 6 participants (6.7%)\n",
      "Basic comprehension: 1 participants (1.1%)\n",
      "\n",
      "Claim Decomposition Metrics:\n",
      "No significant differences found between groups\n",
      "\n",
      "Evidence Synthesis Metrics:\n",
      "No significant differences found between groups\n",
      "\n",
      "Final Conclusion Metrics:\n",
      "No significant differences found between groups\n",
      "\n",
      "Process Evaluation Metrics:\n",
      "\n",
      "Significant differences found:\n",
      "\n",
      "proc_transparency:\n",
      "H-statistic: 8.55\n",
      "p-value: 0.0359\n",
      "\n",
      "Group statistics:\n",
      "  Basic comprehension: 2.00 (n=1)\n",
      "  High proficiency: 3.32 (n=25)\n",
      "  Moderate proficiency: 3.33 (n=6)\n",
      "  Native speaker: 3.72 (n=57)\n",
      "\n",
      "Comparison between groups with sufficient sample size:\n",
      "Range of scores: 3.32 to 3.72\n",
      "\n",
      "Analysis for political_orientation:\n",
      "--------------------------------------------------\n",
      "\n",
      "Group Distribution:\n",
      "Very Liberal: 33 participants (36.7%)\n",
      "Moderately Liberal: 28 participants (31.1%)\n",
      "Moderate: 16 participants (17.8%)\n",
      "Moderately Conservative: 8 participants (8.9%)\n",
      "Very Conservative: 1 participants (1.1%)\n",
      "\n",
      "Claim Decomposition Metrics:\n",
      "No significant differences found between groups\n",
      "\n",
      "Evidence Synthesis Metrics:\n",
      "No significant differences found between groups\n",
      "\n",
      "Final Conclusion Metrics:\n",
      "\n",
      "Significant differences found:\n",
      "\n",
      "fc_support:\n",
      "H-statistic: 13.80\n",
      "p-value: 0.0080\n",
      "\n",
      "Group statistics:\n",
      "  Moderate: 3.41 (n=16)\n",
      "  Moderately Conservative: 3.20 (n=8)\n",
      "  Moderately Liberal: 3.54 (n=28)\n",
      "  Very Conservative: 4.40 (n=1)\n",
      "  Very Liberal: 3.78 (n=33)\n",
      "\n",
      "Comparison between groups with sufficient sample size:\n",
      "Range of scores: 3.20 to 3.78\n",
      "\n",
      "Process Evaluation Metrics:\n",
      "No significant differences found between groups\n",
      "\n",
      "Analysis for fc_years_of_experience:\n",
      "--------------------------------------------------\n",
      "\n",
      "Group Distribution:\n",
      "2-5 years: 36 participants (40.0%)\n",
      "More than 10 years: 22 participants (24.4%)\n",
      "Less than 2 years: 13 participants (14.4%)\n",
      "5-10 years: 12 participants (13.3%)\n",
      "No dedicated fact-checking experience: 7 participants (7.8%)\n",
      "\n",
      "Claim Decomposition Metrics:\n",
      "No significant differences found between groups\n",
      "\n",
      "Evidence Synthesis Metrics:\n",
      "No significant differences found between groups\n",
      "\n",
      "Final Conclusion Metrics:\n",
      "No significant differences found between groups\n",
      "\n",
      "Process Evaluation Metrics:\n",
      "No significant differences found between groups\n"
     ]
    }
   ],
   "execution_count": 179
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T22:13:13.652807Z",
     "start_time": "2025-01-29T22:13:12.478767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# correlation cross metrics\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv(f\"{output_dir}/processed/participants_averages_phase_1.csv\")\n",
    "\n",
    "metrics = {\n",
    "   'Claim Decomposition': [\n",
    "       'cd_overall', 'cd_coverage', 'cd_relevance', 'cd_formulation', 'cd_explainability'\n",
    "   ],\n",
    "   'Evidence Synthesis': [\n",
    "       'es_overall', 'es_relevance', 'es_effectiveness', 'es_logical_connection'\n",
    "   ],\n",
    "   'Final Conclusion': [\n",
    "       'fc_scale5_overall', 'fc_coverage', 'fc_support', 'fc_choice', 'fc_alignment'\n",
    "   ],\n",
    "   'Process Evaluation': [\n",
    "       'proc_scale5_overall', 'proc_explainability', 'proc_transparency', \n",
    "       'proc_sources_transparency', 'proc_trust'\n",
    "   ]\n",
    "}\n",
    "\n",
    "# Create list of all metrics\n",
    "all_metrics = []\n",
    "for phase_metrics in metrics.values():\n",
    "   all_metrics.extend(phase_metrics)\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = df[all_metrics].corr()\n",
    "p_values = pd.DataFrame(np.zeros_like(correlations), columns=correlations.columns, index=correlations.index)\n",
    "\n",
    "# Calculate p-values\n",
    "for i in range(len(correlations.columns)):\n",
    "   for j in range(len(correlations.columns)):\n",
    "       coef, p = stats.spearmanr(df[correlations.columns[i]], df[correlations.columns[j]])\n",
    "       p_values.iloc[i,j] = p\n",
    "\n",
    "# Function to print significant correlations between phases\n",
    "def print_phase_correlations(phase1, phase2):\n",
    "   print(f\"\\nSignificant correlations between {phase1} and {phase2}:\")\n",
    "   print(\"-\" * 50)\n",
    "   \n",
    "   metrics1 = metrics[phase1]\n",
    "   metrics2 = metrics[phase2]\n",
    "   \n",
    "   found_significant = False\n",
    "   \n",
    "   for m1 in metrics1:\n",
    "       for m2 in metrics2:\n",
    "           if m1 != m2:  # Don't report correlation with itself\n",
    "               corr = correlations.loc[m1, m2]\n",
    "               p_val = p_values.loc[m1, m2]\n",
    "               \n",
    "               if p_val < 0.05:  # Significant correlation\n",
    "                   found_significant = True\n",
    "                   print(f\"\\n{m1} vs {m2}:\")\n",
    "                   print(f\"Correlation coefficient: {corr:.2f}\")\n",
    "                   print(f\"P-value: {p_val:.4f}\")\n",
    "                   print(\"Interpretation:\", end=\" \")\n",
    "                   if corr > 0:\n",
    "                       print(f\"Positive correlation - as {m1} increases, {m2} tends to increase\")\n",
    "                   else:\n",
    "                       print(f\"Negative correlation - as {m1} increases, {m2} tends to decrease\")\n",
    "                   \n",
    "                   # Add strength interpretation\n",
    "                   abs_corr = abs(corr)\n",
    "                   if abs_corr < 0.3:\n",
    "                       print(\"Strength: Weak correlation\")\n",
    "                   elif abs_corr < 0.5:\n",
    "                       print(\"Strength: Moderate correlation\")\n",
    "                   else:\n",
    "                       print(\"Strength: Strong correlation\")\n",
    "   \n",
    "   if not found_significant:\n",
    "       print(\"No significant correlations found\")\n",
    "\n",
    "# Print correlations between all pairs of phases\n",
    "phases = list(metrics.keys())\n",
    "for i in range(len(phases)):\n",
    "   for j in range(i + 1, len(phases)):\n",
    "       print_phase_correlations(phases[i], phases[j])\n",
    "\n",
    "# Also create and save correlation heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(correlations, annot=True, cmap='RdBu', center=0, fmt='.2f')\n",
    "plt.title('Correlation Heatmap of All Metrics')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/processed/correlation_heatmap.png\")\n",
    "plt.close()\n",
    "\n",
    "# Save detailed correlations to CSV\n",
    "significant_correlations = []\n",
    "for i in range(len(correlations.columns)):\n",
    "   for j in range(i + 1, len(correlations.columns)):  # Only upper triangle to avoid duplicates\n",
    "       metric1 = correlations.columns[i]\n",
    "       metric2 = correlations.columns[j]\n",
    "       corr = correlations.iloc[i,j]\n",
    "       p_val = p_values.iloc[i,j]\n",
    "       \n",
    "       if p_val < 0.05:  # Significant correlation\n",
    "           significant_correlations.append({\n",
    "               'Metric1': metric1,\n",
    "               'Metric2': metric2,\n",
    "               'Correlation': corr,\n",
    "               'P-value': p_val,\n",
    "               'Strength': 'Strong' if abs(corr) >= 0.5 else 'Moderate' if abs(corr) >= 0.3 else 'Weak',\n",
    "               'Direction': 'Positive' if corr > 0 else 'Negative'\n",
    "           })\n",
    "\n",
    "# Save to CSV\n",
    "pd.DataFrame(significant_correlations).to_csv(f\"{output_dir}/processed/significant_correlations.csv\", index=False)"
   ],
   "id": "cbc25a11f7d14392",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Significant correlations between Claim Decomposition and Evidence Synthesis:\n",
      "--------------------------------------------------\n",
      "\n",
      "cd_overall vs es_overall:\n",
      "Correlation coefficient: 0.74\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_overall increases, es_overall tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_overall vs es_relevance:\n",
      "Correlation coefficient: 0.73\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_overall increases, es_relevance tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_overall vs es_effectiveness:\n",
      "Correlation coefficient: 0.67\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_overall increases, es_effectiveness tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_overall vs es_logical_connection:\n",
      "Correlation coefficient: 0.71\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_overall increases, es_logical_connection tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_coverage vs es_overall:\n",
      "Correlation coefficient: 0.68\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_coverage increases, es_overall tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_coverage vs es_relevance:\n",
      "Correlation coefficient: 0.71\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_coverage increases, es_relevance tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_coverage vs es_effectiveness:\n",
      "Correlation coefficient: 0.60\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_coverage increases, es_effectiveness tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_coverage vs es_logical_connection:\n",
      "Correlation coefficient: 0.62\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_coverage increases, es_logical_connection tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_relevance vs es_overall:\n",
      "Correlation coefficient: 0.68\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_relevance increases, es_overall tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_relevance vs es_relevance:\n",
      "Correlation coefficient: 0.65\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_relevance increases, es_relevance tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_relevance vs es_effectiveness:\n",
      "Correlation coefficient: 0.64\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_relevance increases, es_effectiveness tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_relevance vs es_logical_connection:\n",
      "Correlation coefficient: 0.66\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_relevance increases, es_logical_connection tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_formulation vs es_overall:\n",
      "Correlation coefficient: 0.65\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_formulation increases, es_overall tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_formulation vs es_relevance:\n",
      "Correlation coefficient: 0.64\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_formulation increases, es_relevance tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_formulation vs es_effectiveness:\n",
      "Correlation coefficient: 0.58\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_formulation increases, es_effectiveness tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_formulation vs es_logical_connection:\n",
      "Correlation coefficient: 0.63\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_formulation increases, es_logical_connection tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_explainability vs es_overall:\n",
      "Correlation coefficient: 0.70\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_explainability increases, es_overall tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_explainability vs es_relevance:\n",
      "Correlation coefficient: 0.68\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_explainability increases, es_relevance tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_explainability vs es_effectiveness:\n",
      "Correlation coefficient: 0.62\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_explainability increases, es_effectiveness tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_explainability vs es_logical_connection:\n",
      "Correlation coefficient: 0.70\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_explainability increases, es_logical_connection tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "Significant correlations between Claim Decomposition and Final Conclusion:\n",
      "--------------------------------------------------\n",
      "\n",
      "cd_overall vs fc_scale5_overall:\n",
      "Correlation coefficient: 0.51\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_overall increases, fc_scale5_overall tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_overall vs fc_coverage:\n",
      "Correlation coefficient: 0.55\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_overall increases, fc_coverage tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_overall vs fc_support:\n",
      "Correlation coefficient: 0.52\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_overall increases, fc_support tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_overall vs fc_alignment:\n",
      "Correlation coefficient: 0.29\n",
      "P-value: 0.0175\n",
      "Interpretation: Positive correlation - as cd_overall increases, fc_alignment tends to increase\n",
      "Strength: Weak correlation\n",
      "\n",
      "cd_coverage vs fc_scale5_overall:\n",
      "Correlation coefficient: 0.50\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_coverage increases, fc_scale5_overall tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_coverage vs fc_coverage:\n",
      "Correlation coefficient: 0.52\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_coverage increases, fc_coverage tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_coverage vs fc_support:\n",
      "Correlation coefficient: 0.46\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_coverage increases, fc_support tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_coverage vs fc_alignment:\n",
      "Correlation coefficient: 0.30\n",
      "P-value: 0.0271\n",
      "Interpretation: Positive correlation - as cd_coverage increases, fc_alignment tends to increase\n",
      "Strength: Weak correlation\n",
      "\n",
      "cd_relevance vs fc_scale5_overall:\n",
      "Correlation coefficient: 0.52\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_relevance increases, fc_scale5_overall tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_relevance vs fc_coverage:\n",
      "Correlation coefficient: 0.54\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_relevance increases, fc_coverage tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_relevance vs fc_support:\n",
      "Correlation coefficient: 0.54\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_relevance increases, fc_support tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_relevance vs fc_alignment:\n",
      "Correlation coefficient: 0.29\n",
      "P-value: 0.0177\n",
      "Interpretation: Positive correlation - as cd_relevance increases, fc_alignment tends to increase\n",
      "Strength: Weak correlation\n",
      "\n",
      "cd_formulation vs fc_scale5_overall:\n",
      "Correlation coefficient: 0.31\n",
      "P-value: 0.0036\n",
      "Interpretation: Positive correlation - as cd_formulation increases, fc_scale5_overall tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_formulation vs fc_coverage:\n",
      "Correlation coefficient: 0.39\n",
      "P-value: 0.0007\n",
      "Interpretation: Positive correlation - as cd_formulation increases, fc_coverage tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_formulation vs fc_support:\n",
      "Correlation coefficient: 0.30\n",
      "P-value: 0.0066\n",
      "Interpretation: Positive correlation - as cd_formulation increases, fc_support tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_explainability vs fc_scale5_overall:\n",
      "Correlation coefficient: 0.52\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_explainability increases, fc_scale5_overall tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_explainability vs fc_coverage:\n",
      "Correlation coefficient: 0.55\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_explainability increases, fc_coverage tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_explainability vs fc_support:\n",
      "Correlation coefficient: 0.57\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_explainability increases, fc_support tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "Significant correlations between Claim Decomposition and Process Evaluation:\n",
      "--------------------------------------------------\n",
      "\n",
      "cd_overall vs proc_scale5_overall:\n",
      "Correlation coefficient: 0.46\n",
      "P-value: 0.0001\n",
      "Interpretation: Positive correlation - as cd_overall increases, proc_scale5_overall tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_overall vs proc_explainability:\n",
      "Correlation coefficient: 0.43\n",
      "P-value: 0.0002\n",
      "Interpretation: Positive correlation - as cd_overall increases, proc_explainability tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_overall vs proc_transparency:\n",
      "Correlation coefficient: 0.53\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_overall increases, proc_transparency tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_coverage vs proc_scale5_overall:\n",
      "Correlation coefficient: 0.41\n",
      "P-value: 0.0014\n",
      "Interpretation: Positive correlation - as cd_coverage increases, proc_scale5_overall tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_coverage vs proc_explainability:\n",
      "Correlation coefficient: 0.40\n",
      "P-value: 0.0009\n",
      "Interpretation: Positive correlation - as cd_coverage increases, proc_explainability tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_coverage vs proc_transparency:\n",
      "Correlation coefficient: 0.51\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_coverage increases, proc_transparency tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "cd_relevance vs proc_scale5_overall:\n",
      "Correlation coefficient: 0.44\n",
      "P-value: 0.0003\n",
      "Interpretation: Positive correlation - as cd_relevance increases, proc_scale5_overall tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_relevance vs proc_explainability:\n",
      "Correlation coefficient: 0.39\n",
      "P-value: 0.0009\n",
      "Interpretation: Positive correlation - as cd_relevance increases, proc_explainability tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_relevance vs proc_transparency:\n",
      "Correlation coefficient: 0.48\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_relevance increases, proc_transparency tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_formulation vs proc_scale5_overall:\n",
      "Correlation coefficient: 0.40\n",
      "P-value: 0.0002\n",
      "Interpretation: Positive correlation - as cd_formulation increases, proc_scale5_overall tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_formulation vs proc_explainability:\n",
      "Correlation coefficient: 0.39\n",
      "P-value: 0.0005\n",
      "Interpretation: Positive correlation - as cd_formulation increases, proc_explainability tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_formulation vs proc_transparency:\n",
      "Correlation coefficient: 0.50\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_formulation increases, proc_transparency tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_explainability vs proc_scale5_overall:\n",
      "Correlation coefficient: 0.42\n",
      "P-value: 0.0002\n",
      "Interpretation: Positive correlation - as cd_explainability increases, proc_scale5_overall tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_explainability vs proc_explainability:\n",
      "Correlation coefficient: 0.39\n",
      "P-value: 0.0005\n",
      "Interpretation: Positive correlation - as cd_explainability increases, proc_explainability tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_explainability vs proc_transparency:\n",
      "Correlation coefficient: 0.47\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as cd_explainability increases, proc_transparency tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "cd_explainability vs proc_trust:\n",
      "Correlation coefficient: 0.27\n",
      "P-value: 0.0206\n",
      "Interpretation: Positive correlation - as cd_explainability increases, proc_trust tends to increase\n",
      "Strength: Weak correlation\n",
      "\n",
      "Significant correlations between Evidence Synthesis and Final Conclusion:\n",
      "--------------------------------------------------\n",
      "\n",
      "es_overall vs fc_scale5_overall:\n",
      "Correlation coefficient: 0.52\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as es_overall increases, fc_scale5_overall tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "es_overall vs fc_coverage:\n",
      "Correlation coefficient: 0.59\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as es_overall increases, fc_coverage tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "es_overall vs fc_support:\n",
      "Correlation coefficient: 0.58\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as es_overall increases, fc_support tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "es_relevance vs fc_scale5_overall:\n",
      "Correlation coefficient: 0.53\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as es_relevance increases, fc_scale5_overall tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "es_relevance vs fc_coverage:\n",
      "Correlation coefficient: 0.59\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as es_relevance increases, fc_coverage tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "es_relevance vs fc_support:\n",
      "Correlation coefficient: 0.55\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as es_relevance increases, fc_support tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "es_relevance vs fc_alignment:\n",
      "Correlation coefficient: 0.31\n",
      "P-value: 0.0162\n",
      "Interpretation: Positive correlation - as es_relevance increases, fc_alignment tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "es_effectiveness vs fc_scale5_overall:\n",
      "Correlation coefficient: 0.47\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as es_effectiveness increases, fc_scale5_overall tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "es_effectiveness vs fc_coverage:\n",
      "Correlation coefficient: 0.57\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as es_effectiveness increases, fc_coverage tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "es_effectiveness vs fc_support:\n",
      "Correlation coefficient: 0.55\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as es_effectiveness increases, fc_support tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "es_logical_connection vs fc_scale5_overall:\n",
      "Correlation coefficient: 0.49\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as es_logical_connection increases, fc_scale5_overall tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "es_logical_connection vs fc_coverage:\n",
      "Correlation coefficient: 0.54\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as es_logical_connection increases, fc_coverage tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "es_logical_connection vs fc_support:\n",
      "Correlation coefficient: 0.56\n",
      "P-value: 0.0000\n",
      "Interpretation: Positive correlation - as es_logical_connection increases, fc_support tends to increase\n",
      "Strength: Strong correlation\n",
      "\n",
      "Significant correlations between Evidence Synthesis and Process Evaluation:\n",
      "--------------------------------------------------\n",
      "\n",
      "es_overall vs proc_scale5_overall:\n",
      "Correlation coefficient: 0.33\n",
      "P-value: 0.0095\n",
      "Interpretation: Positive correlation - as es_overall increases, proc_scale5_overall tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "es_overall vs proc_explainability:\n",
      "Correlation coefficient: 0.40\n",
      "P-value: 0.0006\n",
      "Interpretation: Positive correlation - as es_overall increases, proc_explainability tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "es_overall vs proc_transparency:\n",
      "Correlation coefficient: 0.37\n",
      "P-value: 0.0019\n",
      "Interpretation: Positive correlation - as es_overall increases, proc_transparency tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "es_relevance vs proc_scale5_overall:\n",
      "Correlation coefficient: 0.30\n",
      "P-value: 0.0197\n",
      "Interpretation: Positive correlation - as es_relevance increases, proc_scale5_overall tends to increase\n",
      "Strength: Weak correlation\n",
      "\n",
      "es_relevance vs proc_explainability:\n",
      "Correlation coefficient: 0.35\n",
      "P-value: 0.0029\n",
      "Interpretation: Positive correlation - as es_relevance increases, proc_explainability tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "es_relevance vs proc_transparency:\n",
      "Correlation coefficient: 0.35\n",
      "P-value: 0.0035\n",
      "Interpretation: Positive correlation - as es_relevance increases, proc_transparency tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "es_effectiveness vs proc_scale5_overall:\n",
      "Correlation coefficient: 0.35\n",
      "P-value: 0.0069\n",
      "Interpretation: Positive correlation - as es_effectiveness increases, proc_scale5_overall tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "es_effectiveness vs proc_explainability:\n",
      "Correlation coefficient: 0.39\n",
      "P-value: 0.0012\n",
      "Interpretation: Positive correlation - as es_effectiveness increases, proc_explainability tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "es_effectiveness vs proc_transparency:\n",
      "Correlation coefficient: 0.35\n",
      "P-value: 0.0034\n",
      "Interpretation: Positive correlation - as es_effectiveness increases, proc_transparency tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "es_effectiveness vs proc_trust:\n",
      "Correlation coefficient: 0.27\n",
      "P-value: 0.0405\n",
      "Interpretation: Positive correlation - as es_effectiveness increases, proc_trust tends to increase\n",
      "Strength: Weak correlation\n",
      "\n",
      "es_logical_connection vs proc_scale5_overall:\n",
      "Correlation coefficient: 0.31\n",
      "P-value: 0.0115\n",
      "Interpretation: Positive correlation - as es_logical_connection increases, proc_scale5_overall tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "es_logical_connection vs proc_explainability:\n",
      "Correlation coefficient: 0.42\n",
      "P-value: 0.0002\n",
      "Interpretation: Positive correlation - as es_logical_connection increases, proc_explainability tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "es_logical_connection vs proc_transparency:\n",
      "Correlation coefficient: 0.35\n",
      "P-value: 0.0032\n",
      "Interpretation: Positive correlation - as es_logical_connection increases, proc_transparency tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "Significant correlations between Final Conclusion and Process Evaluation:\n",
      "--------------------------------------------------\n",
      "\n",
      "fc_scale5_overall vs proc_transparency:\n",
      "Correlation coefficient: 0.24\n",
      "P-value: 0.0315\n",
      "Interpretation: Positive correlation - as fc_scale5_overall increases, proc_transparency tends to increase\n",
      "Strength: Weak correlation\n",
      "\n",
      "fc_coverage vs proc_scale5_overall:\n",
      "Correlation coefficient: 0.33\n",
      "P-value: 0.0192\n",
      "Interpretation: Positive correlation - as fc_coverage increases, proc_scale5_overall tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "fc_coverage vs proc_explainability:\n",
      "Correlation coefficient: 0.34\n",
      "P-value: 0.0123\n",
      "Interpretation: Positive correlation - as fc_coverage increases, proc_explainability tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "fc_coverage vs proc_transparency:\n",
      "Correlation coefficient: 0.32\n",
      "P-value: 0.0068\n",
      "Interpretation: Positive correlation - as fc_coverage increases, proc_transparency tends to increase\n",
      "Strength: Moderate correlation\n",
      "\n",
      "fc_coverage vs proc_trust:\n",
      "Correlation coefficient: 0.27\n",
      "P-value: 0.0230\n",
      "Interpretation: Positive correlation - as fc_coverage increases, proc_trust tends to increase\n",
      "Strength: Weak correlation\n",
      "\n",
      "fc_support vs proc_transparency:\n",
      "Correlation coefficient: 0.29\n",
      "P-value: 0.0239\n",
      "Interpretation: Positive correlation - as fc_support increases, proc_transparency tends to increase\n",
      "Strength: Weak correlation\n",
      "\n",
      "fc_alignment vs proc_transparency:\n",
      "Correlation coefficient: 0.29\n",
      "P-value: 0.0430\n",
      "Interpretation: Positive correlation - as fc_alignment increases, proc_transparency tends to increase\n",
      "Strength: Weak correlation\n"
     ]
    }
   ],
   "execution_count": 181
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T22:53:23.418760Z",
     "start_time": "2025-01-29T22:53:23.340886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# performance by claim type\n",
    "import scipy\n",
    "\n",
    "claims_df = pd.read_csv(f\"{output_dir}/claims.csv\")\n",
    "cd_df = pd.read_csv(f\"{output_dir}/processed/cd_process_phase_1.csv\")\n",
    "es_df = pd.read_csv(f\"{output_dir}/processed/es_process_phase_1.csv\")\n",
    "fc_df = pd.read_csv(f\"{output_dir}/processed/fc_process_phase_1.csv\")\n",
    "\n",
    "def get_claim_stats(df, claim_number, metric_identifier):\n",
    "    claim_cols = [col for col in df.columns if f'claim{claim_number}' in col and metric_identifier in col]\n",
    "    if claim_cols:\n",
    "        claim_data = df[claim_cols].values.ravel()\n",
    "        return {\n",
    "            'mean': np.mean(claim_data),\n",
    "            'std': np.std(claim_data),\n",
    "            'median': np.median(claim_data),\n",
    "            'n': len(claim_data),\n",
    "            'ci': scipy.stats.t.interval(confidence=0.95, \n",
    "                                 df=len(claim_data)-1,\n",
    "                                 loc=np.mean(claim_data),\n",
    "                                 scale=scipy.stats.sem(claim_data))\n",
    "        }\n",
    "    return None\n",
    "\n",
    "metrics = {\n",
    "    'CD': {\n",
    "        'coverage': 'coverage',\n",
    "        'relevance': 'relevance',\n",
    "        'formulation': 'formulation',\n",
    "        'explainability': 'explainability'\n",
    "    },\n",
    "    'ES': {\n",
    "        'relevance': 'relevance',\n",
    "        'effectiveness': 'effectiveness',\n",
    "        'logical_connection': 'logical_connection'\n",
    "    },\n",
    "    'FC': {\n",
    "        'coverage': 'coverage',\n",
    "        'support': 'support',\n",
    "        'alignment': 'allignment',\n",
    "        'choice': 'choice'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Analyze each claim\n",
    "claim_analysis = []\n",
    "for idx, claim in claims_df.iterrows():\n",
    "    claim_num = claim['number']\n",
    "    claim_data = {\n",
    "        'claim_number': claim_num,\n",
    "        'text': claim['text'],\n",
    "        'political_stance': claim['political_stance']\n",
    "    }\n",
    "    \n",
    "    # Get CD metrics\n",
    "    for metric_name, metric_id in metrics['CD'].items():\n",
    "        stats = get_claim_stats(cd_df, claim_num, metric_id)\n",
    "        if stats:\n",
    "            claim_data[f'cd_{metric_name}_mean'] = stats['mean']\n",
    "            claim_data[f'cd_{metric_name}_ci_lower'] = stats['ci'][0]\n",
    "            claim_data[f'cd_{metric_name}_ci_upper'] = stats['ci'][1]\n",
    "    \n",
    "    # Get ES metrics\n",
    "    for metric_name, metric_id in metrics['ES'].items():\n",
    "        stats = get_claim_stats(es_df, claim_num, metric_id)\n",
    "        if stats:\n",
    "            claim_data[f'es_{metric_name}_mean'] = stats['mean']\n",
    "            claim_data[f'es_{metric_name}_ci_lower'] = stats['ci'][0]\n",
    "            claim_data[f'es_{metric_name}_ci_upper'] = stats['ci'][1]\n",
    "    \n",
    "    # Get FC metrics\n",
    "    for metric_name, metric_id in metrics['FC'].items():\n",
    "        stats = get_claim_stats(fc_df, claim_num, metric_id)\n",
    "        if stats:\n",
    "            claim_data[f'fc_{metric_name}_mean'] = stats['mean']\n",
    "            claim_data[f'fc_{metric_name}_ci_lower'] = stats['ci'][0]\n",
    "            claim_data[f'fc_{metric_name}_ci_upper'] = stats['ci'][1]\n",
    "    \n",
    "    claim_analysis.append(claim_data)\n",
    "\n",
    "# Convert to DataFrame\n",
    "claims_analysis_df = pd.DataFrame(claim_analysis)\n",
    "\n",
    "# Save detailed analysis\n",
    "claims_analysis_df.to_csv(f\"{output_dir}/processed/claims_analysis.csv\", index=False)\n",
    "\n",
    "# Analyze by political stance\n",
    "def print_stance_analysis(df, metric_prefix, metric_name):\n",
    "    print(f\"\\n{metric_prefix.upper()} - {metric_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for stance in df['political_stance'].unique():\n",
    "        stance_data = df[df['political_stance'] == stance]\n",
    "        col_name = f\"{metric_prefix}_{metric_name}_mean\"\n",
    "        mean = stance_data[col_name].mean()\n",
    "        ci = scipy.stats.t.interval(confidence=0.95,\n",
    "                            df=len(stance_data)-1,\n",
    "                            loc=mean,\n",
    "                            scale=scipy.stats.sem(stance_data[col_name]))\n",
    "        print(f\"{stance}:\")\n",
    "        print(f\"Mean: {mean:.2f} (95% CI: [{ci[0]:.2f}, {ci[1]:.2f}])\")\n",
    "        print(f\"N = {len(stance_data)}\")\n",
    "\n",
    "print(\"\\nAnalysis by Political Stance:\")\n",
    "for phase, phase_metrics in metrics.items():\n",
    "    for metric_name in phase_metrics.keys():\n",
    "        print_stance_analysis(claims_analysis_df, phase.lower(), metric_name)\n",
    "\n",
    "print(\"\\nOverall Claim Performance:\")\n",
    "print(\"-\" * 50)\n",
    "for idx, claim in claims_analysis_df.iterrows():\n",
    "    print(f\"\\nClaim {claim['claim_number']} ({claim['political_stance']}):\")\n",
    "    print(f\"Text: {claim['text']}\")\n",
    "    \n",
    "    # Print averages for each phase\n",
    "    for phase in ['cd', 'es', 'fc']:\n",
    "        phase_cols = [col for col in claims_analysis_df.columns if col.startswith(f\"{phase}_\") and col.endswith(\"_mean\")]\n",
    "        if phase_cols:\n",
    "            phase_mean = claim[phase_cols].mean()\n",
    "            print(f\"{phase.upper()} Average: {phase_mean:.2f}\")"
   ],
   "id": "41ecaa3b0ee5691f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis by Political Stance:\n",
      "\n",
      "CD - coverage\n",
      "--------------------------------------------------\n",
      "pro_republican:\n",
      "Mean: 3.87 (95% CI: [2.67, 5.07])\n",
      "N = 2\n",
      "pro_democrat:\n",
      "Mean: 3.89 (95% CI: [nan, nan])\n",
      "N = 1\n",
      "neutral:\n",
      "Mean: 3.96 (95% CI: [3.67, 4.24])\n",
      "N = 2\n",
      "\n",
      "CD - relevance\n",
      "--------------------------------------------------\n",
      "pro_republican:\n",
      "Mean: 3.79 (95% CI: [3.02, 4.57])\n",
      "N = 2\n",
      "pro_democrat:\n",
      "Mean: 3.81 (95% CI: [nan, nan])\n",
      "N = 1\n",
      "neutral:\n",
      "Mean: 3.91 (95% CI: [3.55, 4.26])\n",
      "N = 2\n",
      "\n",
      "CD - formulation\n",
      "--------------------------------------------------\n",
      "pro_republican:\n",
      "Mean: 3.71 (95% CI: [2.02, 5.41])\n",
      "N = 2\n",
      "pro_democrat:\n",
      "Mean: 3.74 (95% CI: [nan, nan])\n",
      "N = 1\n",
      "neutral:\n",
      "Mean: 3.78 (95% CI: [2.93, 4.62])\n",
      "N = 2\n",
      "\n",
      "CD - explainability\n",
      "--------------------------------------------------\n",
      "pro_republican:\n",
      "Mean: 3.86 (95% CI: [2.30, 5.41])\n",
      "N = 2\n",
      "pro_democrat:\n",
      "Mean: 3.82 (95% CI: [nan, nan])\n",
      "N = 1\n",
      "neutral:\n",
      "Mean: 3.91 (95% CI: [3.55, 4.26])\n",
      "N = 2\n",
      "\n",
      "ES - relevance\n",
      "--------------------------------------------------\n",
      "pro_republican:\n",
      "Mean: 3.52 (95% CI: [0.42, 6.63])\n",
      "N = 2\n",
      "pro_democrat:\n",
      "Mean: 3.71 (95% CI: [nan, nan])\n",
      "N = 1\n",
      "neutral:\n",
      "Mean: 3.78 (95% CI: [3.57, 4.00])\n",
      "N = 2\n",
      "\n",
      "ES - effectiveness\n",
      "--------------------------------------------------\n",
      "pro_republican:\n",
      "Mean: 3.55 (95% CI: [1.50, 5.60])\n",
      "N = 2\n",
      "pro_democrat:\n",
      "Mean: 3.53 (95% CI: [nan, nan])\n",
      "N = 1\n",
      "neutral:\n",
      "Mean: 3.72 (95% CI: [2.52, 4.92])\n",
      "N = 2\n",
      "\n",
      "ES - logical_connection\n",
      "--------------------------------------------------\n",
      "pro_republican:\n",
      "Mean: 3.62 (95% CI: [1.99, 5.24])\n",
      "N = 2\n",
      "pro_democrat:\n",
      "Mean: 3.68 (95% CI: [nan, nan])\n",
      "N = 1\n",
      "neutral:\n",
      "Mean: 3.72 (95% CI: [3.36, 4.07])\n",
      "N = 2\n",
      "\n",
      "FC - coverage\n",
      "--------------------------------------------------\n",
      "pro_republican:\n",
      "Mean: 3.54 (95% CI: [0.93, 6.15])\n",
      "N = 2\n",
      "pro_democrat:\n",
      "Mean: 3.37 (95% CI: [nan, nan])\n",
      "N = 1\n",
      "neutral:\n",
      "Mean: 3.73 (95% CI: [3.17, 4.30])\n",
      "N = 2\n",
      "\n",
      "FC - support\n",
      "--------------------------------------------------\n",
      "pro_republican:\n",
      "Mean: 3.45 (95% CI: [-0.43, 7.33])\n",
      "N = 2\n",
      "pro_democrat:\n",
      "Mean: 3.51 (95% CI: [nan, nan])\n",
      "N = 1\n",
      "neutral:\n",
      "Mean: 3.72 (95% CI: [3.44, 4.00])\n",
      "N = 2\n",
      "\n",
      "FC - alignment\n",
      "--------------------------------------------------\n",
      "pro_republican:\n",
      "Mean: 2.75 (95% CI: [1.69, 3.81])\n",
      "N = 2\n",
      "pro_democrat:\n",
      "Mean: 2.59 (95% CI: [nan, nan])\n",
      "N = 1\n",
      "neutral:\n",
      "Mean: 2.99 (95% CI: [0.02, 5.95])\n",
      "N = 2\n",
      "\n",
      "FC - choice\n",
      "--------------------------------------------------\n",
      "pro_republican:\n",
      "Mean: 3.71 (95% CI: [3.35, 4.06])\n",
      "N = 2\n",
      "pro_democrat:\n",
      "Mean: 3.48 (95% CI: [nan, nan])\n",
      "N = 1\n",
      "neutral:\n",
      "Mean: 3.58 (95% CI: [2.17, 4.99])\n",
      "N = 2\n",
      "\n",
      "Overall Claim Performance:\n",
      "--------------------------------------------------\n",
      "\n",
      "Claim 1 (pro_republican):\n",
      "Text: Says Ford agreed to invest $900 million at an Ohio plant because Donald Trump lowered taxes and is now moving the project to Mexico because Joe Biden is increasing taxes.\n",
      "CD Average: 3.91\n",
      "ES Average: 3.74\n",
      "FC Average: 3.50\n",
      "\n",
      "Claim 2 (pro_democrat):\n",
      "Text: The Biden administration inherited gains of 50,000 jobs a month. We're now finally back to 500,000 jobs a month. We inherited a country where 4,000 people a day were dying from Covid. That's now down 75%.\n",
      "CD Average: 3.82\n",
      "ES Average: 3.64\n",
      "FC Average: 3.24\n",
      "\n",
      "Claim 3 (neutral):\n",
      "Text: Officials recommend that women who get one of these (COVID-19) shots should absolutely not get pregnant for at least the first two months after they've been injected.\n",
      "CD Average: 3.91\n",
      "ES Average: 3.77\n",
      "FC Average: 3.60\n",
      "\n",
      "Claim 4 (pro_republican):\n",
      "Text: Joe Biden and Kamala Harris government-run health care plan could lead to hospitals being closed, put Medicare coverage at risk, and give benefits to illegal immigrants.\n",
      "CD Average: 3.71\n",
      "ES Average: 3.39\n",
      "FC Average: 3.22\n",
      "\n",
      "Claim 5 (neutral):\n",
      "Text: Wisconsin was the last state to start paying COVID-related federal unemployment benefits.\n",
      "CD Average: 3.86\n",
      "ES Average: 3.71\n",
      "FC Average: 3.41\n"
     ]
    }
   ],
   "execution_count": 187
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9767477934019fcb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
