# EFFORT: Explainable Framework for Fact-checking Oversight in Real-Time

## Getting started
Clone the repository and install the requirements:
```
pip install -r requirements.txt
```

Corpus datasets, autogenerated sample datasets, models and parameters available via: https://drive.google.com/drive/folders/1KSni3hrkq8MCyQoOURnEEq7yfyY2qd9T?usp=sharing

## Corpus Data format
The corpus data files are formatted as jsonlines. The description of each field is as follows:  

| Field            | type         | Description                                                                                 |
|------------------|--------------|---------------------------------------------------------------------------------------------|
| `example_id`     | string       | Example ID                                                                                  |
| `claim`          | string       | Claim                                                                                       |
| `label`          | string       | Label: pants-fire, false, barely-true, half-true, mostly-true, true                         |
| `person`         | string       | Person who made the claim                                                                   |
| `venue`          | string       | Date and venue of the claim                                                                 |
| `url`            | string       | Politifact URL of the claim                                                                 |
| `category`       | string       | The claim category assigned by a LLM with in-context learning                               |
| `subcategory`    | string       | Specific to the Politics category, a subcategort assigned by a LLM with in-context learning |

## Autogenerated Sample Data format
Additionally to the corpus data format described above, the auto-generated dataset includes the following data fields: 

| Field                                   | type             | Description                                                                  |
|-----------------------------------------|------------------|------------------------------------------------------------------------------|
| `decomposed_questions`                  | List[string]     | The 10 decomposed question for the corresponding claim                       |
| `decomposed_justifications`             | List[string]     | The 10 decomposed justifications for the corresponding decomposed questions  |
| `decomposed_justification_explanations` | List[string]     | The 10 explanation summaries for the corresponding decomposed questions      |
| decomposed_search_hits                  | List[dictionary] | Includes a list with the evidence retrieved for each decomposed question     |

Each `decomposed_search_hits` element is a dictionary with the following fields:
```
decomposed_search_hit = {
    "decomposed_question: "The original decomposed question that initiated the evidence retrieval"
    "decomposed_justification: "The justification to reason the corresponding decomposed question"
    "decomposed_justification_explanation: "The summary explanation for the corresponding decomposed question"
    'pages_info': [
        {"page_name": name of the page included in the search results,
         "page_url": url of the page,
         "page_timestamp": publication timestamp of the page,
         "page_content": scraped content of the page,
         "justification_summary": the page_content summary
        }
        ...
    ]
}
```

## EFFORT Pipeline
Each module below implements a step of EFFORT's pipeline and can be executed separately or as part of the complete workflow:

## Claim Decomp:

To decompopse the claim into a set of sub-questions, you can run module claim_decomposer.py with the following parameters:
--input_path: "string with absolute path to the corpus json dataset"
--output_path: "string with absolute path to the json to store the decomposed dataset"

## Use Bing API to retrieve evidence
To use Bing API, you need to register a Bing API key from [here](https://www.microsoft.com/en-us/bing/apis/bing-web-search-api). Then, you can run the following command to retrieve evidence using Bing:
```
python evidence_retrieval.py \
--input_url data/train.jsonl \
--output_file OUTPUT_FILE_PATH \
--use_time_stamp 1 \
--sites_constrain 1 \
--use_annotation 0 \
--use_claim 0 \
--question_num 10 \
--answer_count 10 \
--chunk_size 50 \
--time_offset 1
```
Check the argument description in `evidence_retrieval.py` for more details.

## Second stage retrieval + summarization
To generate the claim-focused summary, you can run the following command:
```
pyhton python generate_summarization.py \
--input_path ./data/train-site-restricted.jsonl \
--corpus_path ./data/corpus/train.json \
--output_path OUTPUT_FILE_PATH \
--num 5 \
--window_size 30 \
--topk_units 10 \
--topk_docs 5 \
--stride 15 \
--use_claim 0 \
--use_annotation 0 \
--use_justification 0 \
--text_unit span \
--time_restricted 1 \
--time_window 1 \
--summ_type multi_summs \
--use_ChatGPT 0 \
--use_fewshot 0 \
--filter_politifact 0
```
Check the argument description in `generate_summarization.py` for more details.

## Veracity prediction
To predict the veracity of the claim, you can run the following command:
```
bash run_veracity_classification.sh
```
